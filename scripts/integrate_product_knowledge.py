#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Product Knowledge é›†æˆè„šæœ¬
è¿æ¥ Twitter Monitor çš„åˆ†æç»“æœå’Œ Product Knowledge æ•°æ®åº“

è¿™ä¸ªè„šæœ¬çš„ä½œç”¨ï¼š
1. è¯»å– Twitter Monitor å·²ç»åˆ†æå¥½çš„äº§å“æ•°æ®ï¼ˆanalyze_tweets.py çš„è¾“å‡ºï¼‰
2. ä¸ Product Knowledge æ•°æ®åº“å¯¹æ¯”ï¼ŒåŒºåˆ†æ–°æ—§äº§å“
3. å°†æ–°äº§å“æ·»åŠ åˆ° Product Knowledge
4. ç”Ÿæˆå¸¦æœ‰å®Œæ•´äº§å“å…ƒæ•°æ®çš„æŠ¥å‘Š
"""

import json
import sys
from pathlib import Path
from datetime import datetime
from collections import defaultdict

# Product Knowledge è·¯å¾„
PK_PATH = Path("/Users/wenyongteng/vibe_coding/product_knowledge-20251022")
PK_VERSION = "v1_cleaned_20251025"


def load_product_knowledge():
    """åŠ è½½ Product Knowledge æ•°æ®åº“"""
    print("ğŸ“š åŠ è½½ Product Knowledge æ•°æ®åº“...")

    version_path = PK_PATH / "versions" / PK_VERSION
    products_file = version_path / "products_list.json"

    if not products_file.exists():
        print(f"   âš ï¸  æ•°æ®åº“ä¸å­˜åœ¨: {products_file}")
        return {}

    with open(products_file, 'r', encoding='utf-8') as f:
        products = json.load(f)

    # åªä¿ç•™ dict ç±»å‹çš„äº§å“
    products = {k: v for k, v in products.items() if isinstance(v, dict)}

    print(f"   âœ… åŠ è½½äº† {len(products)} ä¸ªäº§å“")
    return products


def load_twitter_analysis(analysis_file):
    """åŠ è½½ Twitter åˆ†æç»“æœ"""
    print(f"ğŸ“Š åŠ è½½ Twitter åˆ†æç»“æœ...")
    print(f"   æ–‡ä»¶: {analysis_file}")

    with open(analysis_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    products = data.get('products', {})
    new_products = data.get('new_products', {})

    print(f"   âœ… åˆ†æä¸­è¯†åˆ«äº† {len(products)} ä¸ªäº§å“")
    print(f"   âœ… å…¶ä¸­ {len(new_products)} ä¸ªæ ‡è®°ä¸ºæ–°äº§å“")

    return data


def classify_products(twitter_products, knowledge_db):
    """
    å¯¹æ¯” Twitter äº§å“å’ŒçŸ¥è¯†åº“ï¼Œåˆ†ç±»ä¸ºï¼šæ–°äº§å“ vs å·²æœ‰äº§å“

    **é‡è¦**: å¤„ç†æ‰€æœ‰ Twitter äº§å“ï¼Œä¸åªæ˜¯ Top 30

    Returns:
        dict: {
            'new_products': [],      # çŸ¥è¯†åº“ä¸­ä¸å­˜åœ¨çš„äº§å“
            'existing_products': [], # çŸ¥è¯†åº“ä¸­å·²æœ‰çš„äº§å“ï¼ˆé™„åŠ å…ƒæ•°æ®ï¼‰
            'ambiguous': []          # æ— æ³•ç¡®å®šçš„äº§å“
        }
    """
    print("\nğŸ” å¯¹æ¯”äº§å“ä¸çŸ¥è¯†åº“...")
    print(f"   - Twitter äº§å“æ•°: {len(twitter_products)}")
    print(f"   - çŸ¥è¯†åº“äº§å“æ•°: {len(knowledge_db)}")

    new_products = []
    existing_products = []
    ambiguous = []

    # åˆ›å»ºçŸ¥è¯†åº“çš„è§„èŒƒåŒ–ç´¢å¼•
    kb_normalized = {}
    for kb_name, kb_data in knowledge_db.items():
        normalized_name = kb_name.lower().strip()
        kb_normalized[normalized_name] = {
            'original_name': kb_name,
            'data': kb_data
        }

        # ä¹Ÿç´¢å¼•åˆ«å
        aliases = kb_data.get('aliases', [])
        for alias in aliases:
            kb_normalized[alias.lower().strip()] = {
                'original_name': kb_name,
                'data': kb_data
            }

    # å¯¹æ¯”æ¯ä¸ª Twitter äº§å“
    for product_name, twitter_data in twitter_products.items():
        normalized = product_name.lower().strip()

        if normalized in kb_normalized:
            # å·²æœ‰äº§å“
            kb_match = kb_normalized[normalized]
            existing_products.append({
                'name': product_name,
                'twitter_data': twitter_data,
                'knowledge_data': kb_match['data'],
                'kb_canonical_name': kb_match['original_name']
            })
        else:
            # æ¨¡ç³ŠåŒ¹é…
            fuzzy_match = None
            for kb_norm, kb_info in kb_normalized.items():
                # ç®€å•çš„åŒ…å«åŒ¹é…
                if normalized in kb_norm or kb_norm in normalized:
                    fuzzy_match = kb_info
                    break

            if fuzzy_match:
                # æ¨¡ç³ŠåŒ¹é…åˆ°äº†
                ambiguous.append({
                    'name': product_name,
                    'twitter_data': twitter_data,
                    'possible_match': fuzzy_match['original_name'],
                    'knowledge_data': fuzzy_match['data']
                })
            else:
                # çœŸæ­£çš„æ–°äº§å“
                new_products.append({
                    'name': product_name,
                    'twitter_data': twitter_data
                })

    print(f"   âœ… åˆ†ç±»å®Œæˆ:")
    print(f"      - æ–°äº§å“: {len(new_products)}")
    print(f"      - å·²æœ‰äº§å“: {len(existing_products)}")
    print(f"      - æ¨¡ç³ŠåŒ¹é…: {len(ambiguous)}")

    return {
        'new_products': new_products,
        'existing_products': existing_products,
        'ambiguous': ambiguous
    }


def update_knowledge_db(new_products):
    """
    å°†æ–°äº§å“æ·»åŠ åˆ° Product Knowledge

    åˆ›å»ºæ–°ç‰ˆæœ¬ï¼Œä¸ä¿®æ”¹ç°æœ‰ç‰ˆæœ¬
    """
    if not new_products:
        print("\n   â„¹ï¸  æ²¡æœ‰æ–°äº§å“ï¼Œè·³è¿‡æ•°æ®åº“æ›´æ–°")
        return None

    print(f"\nğŸ’¾ æ›´æ–° Product Knowledge æ•°æ®åº“...")
    print(f"   æ–°äº§å“æ•°: {len(new_products)}")

    # åŠ è½½å½“å‰ç‰ˆæœ¬
    current_version_path = PK_PATH / "versions" / PK_VERSION
    current_products_file = current_version_path / "products_list.json"

    with open(current_products_file, 'r', encoding='utf-8') as f:
        all_products = json.load(f)

    # åªä¿ç•™ dict ç±»å‹
    all_products = {k: v for k, v in all_products.items() if isinstance(v, dict)}

    original_count = len(all_products)

    # æ·»åŠ æ–°äº§å“
    for product in new_products:
        product_name = product['name']
        twitter_data = product['twitter_data']

        all_products[product_name] = {
            'name': product_name,
            'company': 'Unknown',  # å¯ä»¥ä» Twitter æ•°æ®æ¨æ–­
            'category': 'Unknown',
            'first_seen': datetime.now().isoformat(),
            'mention_count': twitter_data.get('mention_count', 0),
            'source': 'twitter_monitor',
            'aliases': []
        }

    # åˆ›å»ºæ–°ç‰ˆæœ¬
    new_version_name = f"v2_twitter_{datetime.now().strftime('%Y%m%d')}"
    new_version_path = PK_PATH / "versions" / new_version_name
    new_version_path.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜æ–°äº§å“åˆ—è¡¨
    new_products_file = new_version_path / "products_list.json"
    with open(new_products_file, 'w', encoding='utf-8') as f:
        json.dump(all_products, f, indent=2, ensure_ascii=False)

    # ä¿å­˜å…ƒæ•°æ®
    metadata = {
        "version": new_version_name,
        "created_at": datetime.now().isoformat(),
        "based_on": PK_VERSION,
        "type": "twitter_update",
        "changes": {
            "new_products_added": len(new_products),
            "original_product_count": original_count,
            "new_product_count": len(all_products)
        },
        "new_products_list": [p['name'] for p in new_products]
    }

    metadata_file = new_version_path / "metadata.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    print(f"   âœ… æ•°æ®åº“å·²æ›´æ–°:")
    print(f"      - åŸäº§å“æ•°: {original_count}")
    print(f"      - æ–°äº§å“æ•°: {len(all_products)}")
    print(f"      - æ–°ç‰ˆæœ¬: {new_version_name}")
    print(f"      - è·¯å¾„: {new_version_path}")

    return new_version_name


def generate_enhanced_report(twitter_analysis, classification, output_file):
    """
    ç”Ÿæˆå¢å¼ºç‰ˆæŠ¥å‘Šï¼ˆé›†æˆäº†äº§å“çŸ¥è¯†åº“çš„å…ƒæ•°æ®ï¼‰
    """
    print(f"\nğŸ“ ç”Ÿæˆå¢å¼ºç‰ˆæŠ¥å‘Š...")

    report = f"""# Twitter Product Trends Report (Enhanced)
## {twitter_analysis['summary']['date_range']['start']} è‡³ {twitter_analysis['summary']['date_range']['end']}

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**æ•°æ®æ¥æº**: Twitter Monitor + Product Knowledge

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

**æ•°æ®æ¦‚è§ˆ**
- æ€»æ¨æ–‡æ•°: {twitter_analysis['summary']['total_tweets']:,}
- è¯†åˆ«äº§å“: {twitter_analysis['summary']['unique_products']} ä¸ª
  - **ğŸ†• æ–°äº§å“**: {len(classification['new_products'])} ä¸ª
  - **ğŸ“¦ å·²æœ‰äº§å“**: {len(classification['existing_products'])} ä¸ª
  - **â“ æ¨¡ç³ŠåŒ¹é…**: {len(classification['ambiguous'])} ä¸ª

---

## ğŸ†• æ–°äº§å“å‘ç° ({len(classification['new_products'])}ä¸ª)

æœ¬å‘¨é¦–æ¬¡å‡ºç°åœ¨ Twitter è®¨è®ºä¸­çš„äº§å“ï¼š

"""

    for i, product in enumerate(classification['new_products'][:20], 1):
        name = product['name']
        data = product['twitter_data']

        report += f"""### {i}. {name}

**åŸºæœ¬ä¿¡æ¯**
- æåŠæ¬¡æ•°: {data.get('mention_count', 0)} æ¬¡
- æ€»äº’åŠ¨æ•°: {data.get('total_engagement', 0)}
- è®¨è®ºçƒ­åº¦: {'â­' * min(5, data.get('mention_count', 0) // 3 + 1)}

**Top KOLs**
{chr(10).join(f"- @{kol}" for kol in data.get('top_kols', [])[:3])}

**ç¤ºä¾‹æ¨æ–‡**
```
{data.get('sample_tweets', [{}])[0].get('text', 'N/A')[:200]}...
```

---

"""

    report += f"""

## ğŸ“¦ çƒ­é—¨å·²æœ‰äº§å“ Top 30

è¿™äº›äº§å“åœ¨ Product Knowledge æ•°æ®åº“ä¸­å·²å­˜åœ¨ï¼Œæœ¬å‘¨ç»§ç»­æ´»è·ƒï¼š

"""

    for i, product in enumerate(classification['existing_products'][:30], 1):
        name = product['name']
        twitter_data = product['twitter_data']
        kb_data = product['knowledge_data']

        report += f"""### {i}. {product['kb_canonical_name']}

**åŸºæœ¬ä¿¡æ¯** (æ¥è‡ªçŸ¥è¯†åº“)
- å…¬å¸: {kb_data.get('company', 'Unknown')}
- ç±»åˆ«: {kb_data.get('category', 'Unknown')}
- é¦–æ¬¡æ”¶å½•: {kb_data.get('first_seen', 'Unknown')[:10]}

**æœ¬å‘¨åŠ¨æ€**
- æåŠæ¬¡æ•°: {twitter_data.get('mention_count', 0)} æ¬¡
- æ€»äº’åŠ¨æ•°: {twitter_data.get('total_engagement', 0)}
- è®¨è®ºçƒ­åº¦: {'â­' * min(5, twitter_data.get('mention_count', 0) // 3 + 1)}

**æƒ…æ„Ÿåˆ†å¸ƒ**
{dict(twitter_data.get('sentiment', {}))}

---

"""

    # ä¿å­˜æŠ¥å‘Š
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"   âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
    return output_file


def main(twitter_analysis_file, update_db=True):
    """ä¸»æµç¨‹"""

    print("=" * 80)
    print("ğŸš€ Product Knowledge é›†æˆæµç¨‹")
    print("=" * 80)

    # 1. åŠ è½½æ•°æ®
    knowledge_db = load_product_knowledge()
    twitter_analysis = load_twitter_analysis(twitter_analysis_file)

    # 2. åˆ†ç±»äº§å“
    classification = classify_products(
        twitter_analysis.get('products', {}),
        knowledge_db
    )

    # 3. æ›´æ–°æ•°æ®åº“
    new_version = None
    if update_db and classification['new_products']:
        new_version = update_knowledge_db(classification['new_products'])

    # 4. ç”ŸæˆæŠ¥å‘Š
    output_file = twitter_analysis_file.replace('analysis_summary.json', 'enhanced_report.md')
    report_path = generate_enhanced_report(twitter_analysis, classification, output_file)

    # 5. ä¿å­˜åˆ†ç±»ç»“æœ
    classification_file = twitter_analysis_file.replace('analysis_summary.json', 'product_classification.json')
    with open(classification_file, 'w', encoding='utf-8') as f:
        json.dump(classification, f, indent=2, ensure_ascii=False)

    print("\n" + "=" * 80)
    print("âœ… é›†æˆå®Œæˆ!")
    print("=" * 80)
    print(f"ğŸ“Š æ–°äº§å“: {len(classification['new_products'])}")
    print(f"ğŸ“¦ å·²æœ‰äº§å“: {len(classification['existing_products'])}")
    print(f"â“ æ¨¡ç³ŠåŒ¹é…: {len(classification['ambiguous'])}")
    if new_version:
        print(f"ğŸ’¾ æ•°æ®åº“ç‰ˆæœ¬: {new_version}")
    print(f"ğŸ“„ å¢å¼ºæŠ¥å‘Š: {report_path}")
    print(f"ğŸ“„ åˆ†ç±»ç»“æœ: {classification_file}")
    print("=" * 80)

    return {
        'classification': classification,
        'new_version': new_version,
        'report_path': report_path
    }


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='é›†æˆ Twitter åˆ†æå’Œ Product Knowledge')
    parser.add_argument('analysis_file', help='Twitter åˆ†æç»“æœæ–‡ä»¶ (analysis_summary.json)')
    parser.add_argument('--no-update-db', action='store_true', help='ä¸æ›´æ–°æ•°æ®åº“')

    args = parser.parse_args()

    result = main(
        twitter_analysis_file=args.analysis_file,
        update_db=not args.no_update_db
    )
