#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Product Processor - Module 2
è°ƒç”¨ product_knowledge æå–å’ŒåŒ¹é…äº§å“ä¿¡æ¯
"""

import json
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
from collections import defaultdict
import time

# æ·»åŠ  product_knowledge åˆ° Python Path
PRODUCT_KNOWLEDGE_PATH = Path("/Users/wenyongteng/vibe_coding/product_knowledge-20251022")
sys.path.insert(0, str(PRODUCT_KNOWLEDGE_PATH))

try:
    from openai import OpenAI
except ImportError:
    print("âŒ éœ€è¦å®‰è£… openai åº“")
    print("   pip3 install openai")
    sys.exit(1)


class ProductProcessor:
    """äº§å“æå–å’ŒåŒ¹é…å¤„ç†å™¨"""

    def __init__(self, config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–å¤„ç†å™¨

        Args:
            config: é…ç½®å­—å…¸
        """
        if config is None:
            config_path = Path(__file__).parent.parent / "config" / "integration_config.json"
            with open(config_path, 'r', encoding='utf-8') as f:
                full_config = json.load(f)
                extraction_config = full_config['extraction']
                pk_config = full_config['product_knowledge']
                self.classification_config = full_config.get('classification', {})

        else:
            extraction_config = config.get('extraction', {})
            pk_config = config.get('product_knowledge', {})
            self.classification_config = config.get('classification', {})

        # æå–é…ç½®
        self.model = extraction_config.get('model', 'openai/gpt-4o')
        self.api_key = extraction_config.get('api_key')
        self.base_url = extraction_config.get('base_url', 'https://openrouter.ai/api/v1')
        self.batch_size = extraction_config.get('batch_size', 10)
        self.max_workers = extraction_config.get('max_workers', 8)
        self.rate_limit_delay = extraction_config.get('rate_limit_delay', 0.5)

        # Product Knowledge é…ç½®
        self.pk_project_path = Path(pk_config.get('project_path'))
        self.pk_current_version = pk_config.get('current_version')
        self.pk_versions_dir = Path(pk_config.get('versions_dir'))

        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        self.client = OpenAI(
            base_url=self.base_url,
            api_key=self.api_key
        )

        # åŠ è½½ç°æœ‰äº§å“æ•°æ®åº“
        self.existing_products = self._load_existing_products()

        # æå–ç»“æœ
        self.extraction_result = None

    def _load_existing_products(self) -> Dict:
        """åŠ è½½ç°æœ‰çš„äº§å“æ•°æ®åº“"""
        print(f"ğŸ“š åŠ è½½ç°æœ‰äº§å“æ•°æ®åº“...")

        version_path = self.pk_versions_dir / self.pk_current_version
        products_file = version_path / "products_list.json"

        if not products_file.exists():
            print(f"   âš ï¸  æœªæ‰¾åˆ°äº§å“æ•°æ®åº“: {products_file}")
            return {}

        with open(products_file, 'r', encoding='utf-8') as f:
            products = json.load(f)

        print(f"   âœ… åŠ è½½å®Œæˆ: {len(products)} ä¸ªäº§å“")

        return products

    def process(self, tweets: List[Dict]) -> Dict:
        """
        å¤„ç†æ¨æ–‡,æå–å’ŒåŒ¹é…äº§å“

        Args:
            tweets: æ¨æ–‡åˆ—è¡¨

        Returns:
            æå–ç»“æœ
        """
        print(f"\nğŸ” å¼€å§‹æå–å’ŒåŒ¹é…äº§å“...")
        print(f"   - æ¨æ–‡æ•°: {len(tweets)}")
        print(f"   - æ‰¹å¤„ç†å¤§å°: {self.batch_size}")

        # æå–äº§å“
        extracted_products = self._extract_products(tweets)

        # åŒ¹é…ç°æœ‰äº§å“
        matched_results = self._match_products(extracted_products, tweets)

        # ä¿å­˜ç»“æœ
        self.extraction_result = {
            "extraction_metadata": {
                "timestamp": datetime.now().isoformat(),
                "total_tweets": len(tweets),
                "model_used": self.model
            },
            "summary": {
                "total_products_extracted": len(extracted_products),
                "new_products": len(matched_results['new_products']),
                "matched_existing": len(matched_results['matched_products']),
                "new_releases": len(matched_results['new_releases'])
            },
            "products": extracted_products,
            "new_products": matched_results['new_products'],
            "matched_products": matched_results['matched_products'],
            "new_releases": matched_results['new_releases'],
            "product_tweet_map": matched_results['product_tweet_map']
        }

        # ä¿å­˜åˆ°æ–‡ä»¶
        self._save_extraction_result()

        print(f"\n   âœ… æå–å®Œæˆ:")
        print(f"      - æ€»äº§å“æ•°: {len(extracted_products)}")
        print(f"      - æ–°äº§å“: {len(matched_results['new_products'])}")
        print(f"      - å·²æœ‰äº§å“: {len(matched_results['matched_products'])}")
        print(f"      - æ–°ç‰ˆæœ¬: {len(matched_results['new_releases'])}")

        return self.extraction_result

    def _extract_products(self, tweets: List[Dict]) -> List[Dict]:
        """ä½¿ç”¨ GPT-4o æå–äº§å“ä¿¡æ¯"""

        print(f"\n   ğŸ¤– ä½¿ç”¨ {self.model} æå–äº§å“...")

        all_products = []
        product_names_seen = set()

        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(tweets), self.batch_size):
            batch = tweets[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1
            total_batches = (len(tweets) + self.batch_size - 1) // self.batch_size

            print(f"      æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} æ¡æ¨æ–‡)...")

            # æ„å»ºæ‰¹æ¬¡æ–‡æœ¬
            batch_text = self._build_batch_text(batch)

            # è°ƒç”¨ LLM æå–
            products = self._call_llm_extract(batch_text, batch)

            # å»é‡
            for product in products:
                product_name = product.get('name', '').lower()
                if product_name and product_name not in product_names_seen:
                    all_products.append(product)
                    product_names_seen.add(product_name)

            # é€Ÿç‡é™åˆ¶
            if i + self.batch_size < len(tweets):
                time.sleep(self.rate_limit_delay)

        print(f"   âœ… æå–å®Œæˆ: {len(all_products)} ä¸ªä¸åŒäº§å“")

        return all_products

    def _build_batch_text(self, tweets: List[Dict]) -> str:
        """æ„å»ºæ‰¹æ¬¡æ–‡æœ¬"""
        batch_lines = []

        for idx, tweet in enumerate(tweets, 1):
            text = tweet.get('text', '')
            batch_lines.append(f"[Tweet {idx}] {text}")

        return "\n\n".join(batch_lines)

    def _call_llm_extract(self, batch_text: str, tweets: List[Dict]) -> List[Dict]:
        """è°ƒç”¨ LLM æå–äº§å“"""

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äº§å“ä¿¡æ¯æå–åŠ©æ‰‹ã€‚è¯·ä»ä»¥ä¸‹æ¨æ–‡ä¸­æå–æ‰€æœ‰æåˆ°çš„**æŠ€æœ¯äº§å“ã€å·¥å…·ã€æœåŠ¡ã€å¹³å°æˆ–åº”ç”¨**ã€‚

è¦æ±‚:
1. åªæå–æ˜ç¡®çš„äº§å“åç§°(å¦‚ Claude, ChatGPT, VS Code, Cursorç­‰)
2. ä¸è¦æå–:
   - å…¬å¸å(é™¤éå…¬å¸åæœ¬èº«å°±æ˜¯äº§å“å,å¦‚ OpenAI)
   - é€šç”¨æ¦‚å¿µ(å¦‚ AI, machine learning, API)
   - åŠ¨è¯æˆ–å½¢å®¹è¯(å¦‚ launched, released, drop)
3. å¦‚æœäº§å“æœ‰ç‰ˆæœ¬å·,è¯·åœ¨ version å­—æ®µè®°å½•
4. å°½å¯èƒ½æ¨æ–­äº§å“çš„å…¬å¸å’Œç±»åˆ«

æ¨æ–‡å†…å®¹:
{batch_text}

è¯·ä»¥JSONæ ¼å¼è¿”å›,æ ¼å¼å¦‚ä¸‹:
[
  {{
    "name": "äº§å“åç§°",
    "company": "å…¬å¸å(å¦‚æœèƒ½æ¨æ–­å‡º)",
    "category": "äº§å“ç±»åˆ«(å¦‚: AI Tool, IDE, Design Toolç­‰)",
    "version": "ç‰ˆæœ¬å·(å¦‚æœæåˆ°)",
    "mentioned_in_tweet_indices": [1, 3]
  }}
]

æ³¨æ„: åªè¿”å›JSONæ•°ç»„,ä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=2000
            )

            content = response.choices[0].message.content.strip()

            # æå–JSON
            if content.startswith("```json"):
                content = content[7:]
            if content.startswith("```"):
                content = content[3:]
            if content.endswith("```"):
                content = content[:-3]

            content = content.strip()

            products = json.loads(content)

            # æ·»åŠ æ¨æ–‡å¼•ç”¨
            for product in products:
                tweet_indices = product.get('mentioned_in_tweet_indices', [])
                product['related_tweets'] = [
                    tweets[idx - 1] for idx in tweet_indices
                    if 0 < idx <= len(tweets)
                ]

            return products

        except Exception as e:
            print(f"      âš ï¸  æå–å¤±è´¥: {e}")
            return []

    def _match_products(self, extracted_products: List[Dict], tweets: List[Dict]) -> Dict:
        """åŒ¹é…ç°æœ‰äº§å“"""

        print(f"\n   ğŸ”— åŒ¹é…ç°æœ‰äº§å“...")

        new_products = []
        matched_products = []
        new_releases = []
        product_tweet_map = defaultdict(list)

        for product in extracted_products:
            product_name = product.get('name', '').lower()
            version = product.get('version')

            # æ£€æŸ¥æ˜¯å¦åœ¨ç°æœ‰æ•°æ®åº“ä¸­
            existing = None
            for existing_name, existing_data in self.existing_products.items():
                # è·³è¿‡éå­—å…¸ç±»å‹çš„å€¼
                if not isinstance(existing_data, dict):
                    continue

                if existing_name.lower() == product_name:
                    existing = existing_data
                    break

                # æ£€æŸ¥åˆ«å
                aliases = existing_data.get('aliases', [])
                if product_name in [a.lower() for a in aliases]:
                    existing = existing_data
                    break

            if existing:
                # å·²æœ‰äº§å“
                matched_products.append({
                    **product,
                    'existing_data': existing,
                    'match_type': 'exact' if existing_name.lower() == product_name else 'alias'
                })

                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°ç‰ˆæœ¬
                if version:
                    existing_versions = existing.get('versions', [])
                    if version not in existing_versions:
                        new_releases.append({
                            'product_name': product.get('name'),
                            'version': version,
                            'company': product.get('company') or existing.get('company'),
                            'category': product.get('category') or existing.get('category'),
                            'related_tweets': product.get('related_tweets', [])
                        })
            else:
                # æ–°äº§å“
                new_products.append(product)

            # è®°å½•äº§å“-æ¨æ–‡æ˜ å°„
            for tweet in product.get('related_tweets', []):
                product_tweet_map[product.get('name')].append(tweet)

        print(f"   âœ… åŒ¹é…å®Œæˆ:")
        print(f"      - æ–°äº§å“: {len(new_products)}")
        print(f"      - å·²æœ‰äº§å“: {len(matched_products)}")
        print(f"      - æ–°ç‰ˆæœ¬: {len(new_releases)}")

        return {
            'new_products': new_products,
            'matched_products': matched_products,
            'new_releases': new_releases,
            'product_tweet_map': dict(product_tweet_map)
        }

    def _save_extraction_result(self):
        """ä¿å­˜æå–ç»“æœ"""
        output_dir = Path(__file__).parent.parent / "data_sources"
        output_dir.mkdir(parents=True, exist_ok=True)

        date_str = datetime.now().strftime("%Y%m%d")
        output_file = output_dir / f"{date_str}_extracted_products.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.extraction_result, f, indent=2, ensure_ascii=False)

        print(f"\n   ğŸ’¾ æå–ç»“æœå·²ä¿å­˜: {output_file}")

    def update_knowledge_db(self):
        """æ›´æ–° Product Knowledge æ•°æ®åº“"""
        print(f"\nğŸ’¾ æ›´æ–° Product Knowledge æ•°æ®åº“...")

        if not self.extraction_result:
            print("   âš ï¸  æ²¡æœ‰æå–ç»“æœ,è·³è¿‡æ›´æ–°")
            return

        new_products = self.extraction_result.get('new_products', [])

        if not new_products:
            print("   â„¹ï¸  æ²¡æœ‰æ–°äº§å“,æ— éœ€æ›´æ–°æ•°æ®åº“")
            return

        # åˆ›å»ºæ–°ç‰ˆæœ¬
        new_version_name = f"v2_{datetime.now().strftime('%Y%m%d')}"
        new_version_path = self.pk_versions_dir / new_version_name

        print(f"   ğŸ“‚ åˆ›å»ºæ–°ç‰ˆæœ¬: {new_version_name}")
        new_version_path.mkdir(parents=True, exist_ok=True)

        # å¤åˆ¶å½“å‰ç‰ˆæœ¬çš„æ•°æ®
        current_version_path = self.pk_versions_dir / self.pk_current_version
        current_products_file = current_version_path / "products_list.json"

        with open(current_products_file, 'r', encoding='utf-8') as f:
            all_products = json.load(f)

        # æ·»åŠ æ–°äº§å“
        for product in new_products:
            product_name = product.get('name')
            all_products[product_name] = {
                'name': product_name,
                'company': product.get('company', 'Unknown'),
                'category': product.get('category', 'Unknown'),
                'first_seen': datetime.now().isoformat(),
                'version': product.get('version'),
                'aliases': [],
                'source': 'twitter_extraction'
            }

        # ä¿å­˜æ–°ç‰ˆæœ¬
        new_products_file = new_version_path / "products_list.json"
        with open(new_products_file, 'w', encoding='utf-8') as f:
            json.dump(all_products, f, indent=2, ensure_ascii=False)

        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            "version": new_version_name,
            "created_at": datetime.now().isoformat(),
            "based_on": self.pk_current_version,
            "type": "twitter_update",
            "changes": {
                "new_products_added": len(new_products),
                "original_product_count": len(self.existing_products),
                "new_product_count": len(all_products)
            }
        }

        metadata_file = new_version_path / "metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

        print(f"   âœ… æ•°æ®åº“å·²æ›´æ–°:")
        print(f"      - æ–°å¢äº§å“: {len(new_products)}")
        print(f"      - æ€»äº§å“æ•°: {len(all_products)}")
        print(f"      - æ–°ç‰ˆæœ¬è·¯å¾„: {new_version_path}")


def main():
    """æµ‹è¯•å…¥å£"""
    # åŠ è½½æµ‹è¯•æ•°æ®
    data_file = Path(__file__).parent.parent / "data_sources" / "20251025_raw_tweets.json"

    if not data_file.exists():
        print(f"âŒ æµ‹è¯•æ•°æ®ä¸å­˜åœ¨: {data_file}")
        return

    with open(data_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    tweets = data.get('tweets', [])[:50]  # åªæµ‹è¯•å‰50æ¡

    processor = ProductProcessor()
    result = processor.process(tweets)

    print(f"\nâœ… å¤„ç†å®Œæˆ!")
    print(f"   æå–äº§å“: {result['summary']['total_products_extracted']}")
    print(f"   æ–°äº§å“: {result['summary']['new_products']}")


if __name__ == "__main__":
    main()
