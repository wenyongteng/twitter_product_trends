#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Product Knowledge é›†æˆè„šæœ¬ v2 - ä¿®å¤ç‰ˆ
ä¿®å¤ï¼š
1. æ­£ç¡®åŠ è½½ Product Knowledge æ•°æ®åº“ï¼ˆæ”¯æŒåˆ—è¡¨æ ¼å¼ï¼‰
2. å¤„ç†æ‰€æœ‰ 643 ä¸ªäº§å“ï¼Œä¸åªæ˜¯ Top 30
"""

import json
import sys
from pathlib import Path
from datetime import datetime
from collections import defaultdict

# Product Knowledge è·¯å¾„
PK_PATH = Path("/Users/wenyongteng/vibe_coding/product_knowledge-20251022")
PK_VERSION = "v1_cleaned_20251025"


def load_product_knowledge():
    """åŠ è½½ Product Knowledge æ•°æ®åº“ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰"""
    print("ğŸ“š åŠ è½½ Product Knowledge æ•°æ®åº“...")

    version_path = PK_PATH / "versions" / PK_VERSION
    products_file = version_path / "products_list.json"

    if not products_file.exists():
        print(f"   âš ï¸  æ•°æ®åº“ä¸å­˜åœ¨: {products_file}")
        return {}

    with open(products_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # å¤„ç†ä¸åŒæ ¼å¼çš„æ•°æ®
    products_dict = {}

    if isinstance(data, dict):
        if 'products' in data and isinstance(data['products'], list):
            # æ ¼å¼: {"total_products": N, "products": [{...}, ...]}
            print(f"   æ£€æµ‹åˆ°åˆ—è¡¨æ ¼å¼ï¼Œå…± {data.get('total_products', len(data['products']))} ä¸ªäº§å“")
            for product in data['products']:
                if isinstance(product, dict) and 'name' in product:
                    name = product['name']
                    products_dict[name] = product
        else:
            # æ ¼å¼: {"product_name": {...}, ...}
            for k, v in data.items():
                if isinstance(v, dict):
                    products_dict[k] = v

    elif isinstance(data, list):
        # æ ¼å¼: [{"name": "...", ...}, ...]
        for product in data:
            if isinstance(product, dict) and 'name' in product:
                name = product['name']
                products_dict[name] = product

    print(f"   âœ… æˆåŠŸåŠ è½½ {len(products_dict)} ä¸ªäº§å“")

    # æ˜¾ç¤ºç¤ºä¾‹
    if products_dict:
        sample = list(products_dict.keys())[:5]
        print(f"   ç¤ºä¾‹äº§å“: {', '.join(sample)}")

    return products_dict


def load_twitter_analysis(analysis_file):
    """åŠ è½½ Twitter åˆ†æç»“æœï¼ˆæ‰€æœ‰äº§å“ï¼Œä¸åªæ˜¯ Top 30ï¼‰"""
    print(f"\nğŸ“Š åŠ è½½ Twitter åˆ†æç»“æœ...")
    print(f"   æ–‡ä»¶: {analysis_file}")

    with open(analysis_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # è·å–æ‰€æœ‰äº§å“ï¼ˆä¸é™åˆ¶æ•°é‡ï¼‰
    products = data.get('products', {})
    new_products = data.get('new_products', {})

    print(f"   âœ… åˆ†æä¸­è¯†åˆ«äº† {len(products)} ä¸ªäº§å“")
    print(f"   âœ… å…¶ä¸­ {len(new_products)} ä¸ªæ ‡è®°ä¸ºæ–°äº§å“")

    return data


def classify_products(twitter_products, knowledge_db):
    """
    å¯¹æ¯” Twitter äº§å“å’ŒçŸ¥è¯†åº“ï¼Œåˆ†ç±»ä¸ºï¼šæ–°äº§å“ vs å·²æœ‰äº§å“
    å¤„ç†**æ‰€æœ‰** Twitter äº§å“
    """
    print("\nğŸ” å¯¹æ¯”äº§å“ä¸çŸ¥è¯†åº“...")
    print(f"   - Twitter äº§å“æ•°: {len(twitter_products)}")
    print(f"   - çŸ¥è¯†åº“äº§å“æ•°: {len(knowledge_db)}")

    new_products = []
    existing_products = []
    ambiguous = []

    # åˆ›å»ºçŸ¥è¯†åº“çš„è§„èŒƒåŒ–ç´¢å¼•ï¼ˆåŒ…æ‹¬åˆ«åï¼‰
    kb_normalized = {}
    for kb_name, kb_data in knowledge_db.items():
        if not isinstance(kb_data, dict):
            continue

        normalized_name = kb_name.lower().strip()
        kb_normalized[normalized_name] = {
            'original_name': kb_name,
            'data': kb_data
        }

        # ç´¢å¼•åˆ«åï¼ˆå¦‚æœæœ‰ï¼‰
        aliases = kb_data.get('aliases', [])
        if not isinstance(aliases, list):
            aliases = []

        for alias in aliases:
            if alias:
                kb_normalized[alias.lower().strip()] = {
                    'original_name': kb_name,
                    'data': kb_data
                }

    print(f"   - çŸ¥è¯†åº“ç´¢å¼•ï¼ˆå«åˆ«åï¼‰: {len(kb_normalized)} ä¸ªæ¡ç›®")

    # å¯¹æ¯”æ¯ä¸ª Twitter äº§å“
    for product_name, twitter_data in twitter_products.items():
        normalized = product_name.lower().strip()

        if normalized in kb_normalized:
            # ç²¾ç¡®åŒ¹é…ï¼šå·²æœ‰äº§å“
            kb_match = kb_normalized[normalized]
            existing_products.append({
                'name': product_name,
                'twitter_data': twitter_data,
                'knowledge_data': kb_match['data'],
                'kb_canonical_name': kb_match['original_name']
            })
        else:
            # æ¨¡ç³ŠåŒ¹é…ï¼ˆåŒ…å«å…³ç³»ï¼‰
            fuzzy_match = None
            for kb_norm, kb_info in kb_normalized.items():
                if normalized in kb_norm or kb_norm in normalized:
                    fuzzy_match = kb_info
                    break

            if fuzzy_match:
                # æ¨¡ç³ŠåŒ¹é…
                ambiguous.append({
                    'name': product_name,
                    'twitter_data': twitter_data,
                    'possible_match': fuzzy_match['original_name'],
                    'knowledge_data': fuzzy_match['data']
                })
            else:
                # çœŸæ­£çš„æ–°äº§å“
                new_products.append({
                    'name': product_name,
                    'twitter_data': twitter_data
                })

    print(f"   âœ… åˆ†ç±»å®Œæˆ:")
    print(f"      - æ–°äº§å“: {len(new_products)}")
    print(f"      - å·²æœ‰äº§å“ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰: {len(existing_products)}")
    print(f"      - æ¨¡ç³ŠåŒ¹é…: {len(ambiguous)}")

    return {
        'new_products': new_products,
        'existing_products': existing_products,
        'ambiguous': ambiguous
    }


def update_knowledge_db(new_products):
    """å°†æ–°äº§å“æ·»åŠ åˆ° Product Knowledge"""
    if not new_products:
        print("\n   â„¹ï¸  æ²¡æœ‰æ–°äº§å“ï¼Œè·³è¿‡æ•°æ®åº“æ›´æ–°")
        return None

    print(f"\nğŸ’¾ æ›´æ–° Product Knowledge æ•°æ®åº“...")
    print(f"   æ–°äº§å“æ•°: {len(new_products)}")

    # åŠ è½½å½“å‰ç‰ˆæœ¬
    current_version_path = PK_PATH / "versions" / PK_VERSION
    current_products_file = current_version_path / "products_list.json"

    with open(current_products_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # å¤„ç†ä¸åŒæ ¼å¼
    if isinstance(data, dict) and 'products' in data:
        products_list = data['products']
        original_count = len(products_list)
    else:
        products_list = []
        original_count = 0

    # æ·»åŠ æ–°äº§å“
    next_id = max([p.get('id', 0) for p in products_list], default=0) + 1

    for product in new_products:
        product_name = product['name']
        twitter_data = product['twitter_data']

        new_product = {
            'id': next_id,
            'name': product_name,
            'company': 'Unknown',
            'versions': [],
            'mention_count': twitter_data.get('mention_count', 0),
            'first_mention_time': datetime.now().isoformat(),
            'source': 'twitter_monitor',
            'confidence': 0.8
        }

        products_list.append(new_product)
        next_id += 1

    # åˆ›å»ºæ–°ç‰ˆæœ¬
    new_version_name = f"v2_twitter_{datetime.now().strftime('%Y%m%d_%H%M')}"
    new_version_path = PK_PATH / "versions" / new_version_name
    new_version_path.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜æ–°äº§å“åˆ—è¡¨
    new_data = {
        "total_products": len(products_list),
        "products": products_list
    }

    new_products_file = new_version_path / "products_list.json"
    with open(new_products_file, 'w', encoding='utf-8') as f:
        json.dump(new_data, f, indent=2, ensure_ascii=False)

    # ä¿å­˜å…ƒæ•°æ®
    metadata = {
        "version": new_version_name,
        "created_at": datetime.now().isoformat(),
        "based_on": PK_VERSION,
        "type": "twitter_update",
        "changes": {
            "new_products_added": len(new_products),
            "original_product_count": original_count,
            "new_product_count": len(products_list)
        },
        "new_products_list": [p['name'] for p in new_products]
    }

    metadata_file = new_version_path / "metadata.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    print(f"   âœ… æ•°æ®åº“å·²æ›´æ–°:")
    print(f"      - åŸäº§å“æ•°: {original_count}")
    print(f"      - æ–°äº§å“æ•°: {len(products_list)}")
    print(f"      - æ–°ç‰ˆæœ¬: {new_version_name}")
    print(f"      - è·¯å¾„: {new_version_path}")

    return new_version_name


def generate_enhanced_report(twitter_analysis, classification, output_file):
    """ç”Ÿæˆå¢å¼ºç‰ˆæŠ¥å‘Š"""
    print(f"\nğŸ“ ç”Ÿæˆå¢å¼ºç‰ˆæŠ¥å‘Š...")

    report = f"""# Twitter Product Trends Report (Enhanced)
## {twitter_analysis['summary']['date_range']['start']} è‡³ {twitter_analysis['summary']['date_range']['end']}

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**æ•°æ®æ¥æº**: Twitter Monitor + Product Knowledge

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

**æ•°æ®æ¦‚è§ˆ**
- æ€»æ¨æ–‡æ•°: {twitter_analysis['summary']['total_tweets']:,}
- è¯†åˆ«äº§å“: {twitter_analysis['summary']['unique_products']} ä¸ª
  - **ğŸ†• æ–°äº§å“**: {len(classification['new_products'])} ä¸ª
  - **ğŸ“¦ å·²æœ‰äº§å“ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰**: {len(classification['existing_products'])} ä¸ª
  - **â“ æ¨¡ç³ŠåŒ¹é…**: {len(classification['ambiguous'])} ä¸ª

---

## ğŸ†• æ–°äº§å“å‘ç° ({len(classification['new_products'])}ä¸ª)

è¿™äº›äº§å“åœ¨ Product Knowledge æ•°æ®åº“ä¸­ä¸å­˜åœ¨ï¼š

"""

    # æ–°äº§å“åˆ—è¡¨
    for i, product in enumerate(sorted(classification['new_products'],
                                      key=lambda x: x['twitter_data'].get('mention_count', 0),
                                      reverse=True)[:50], 1):
        name = product['name']
        data = product['twitter_data']

        report += f"""### {i}. {name}

- æåŠæ¬¡æ•°: {data.get('mention_count', 0)} æ¬¡
- æ€»äº’åŠ¨æ•°: {data.get('total_engagement', 0)}
- Top KOLs: {', '.join([f"@{k}" for k in data.get('top_kols', [])[:3]])}

---

"""

    # å·²æœ‰äº§å“åˆ—è¡¨
    report += f"""

## ğŸ“¦ å·²æœ‰äº§å“ ({len(classification['existing_products'])}ä¸ª)

è¿™äº›äº§å“åœ¨ Product Knowledge æ•°æ®åº“ä¸­å·²å­˜åœ¨ï¼š

"""

    for i, product in enumerate(sorted(classification['existing_products'],
                                      key=lambda x: x['twitter_data'].get('mention_count', 0),
                                      reverse=True)[:50], 1):
        name = product['kb_canonical_name']
        twitter_data = product['twitter_data']
        kb_data = product['knowledge_data']

        report += f"""### {i}. {name}

**çŸ¥è¯†åº“ä¿¡æ¯**
- å…¬å¸: {kb_data.get('company', 'Unknown')}
- å†å²æåŠ: {kb_data.get('mention_count', 0)} æ¬¡

**æœ¬å‘¨æ•°æ®**
- æåŠæ¬¡æ•°: {twitter_data.get('mention_count', 0)} æ¬¡
- æ€»äº’åŠ¨æ•°: {twitter_data.get('total_engagement', 0)}

---

"""

    # æ¨¡ç³ŠåŒ¹é…
    if classification['ambiguous']:
        report += f"""

## â“ æ¨¡ç³ŠåŒ¹é… ({len(classification['ambiguous'])}ä¸ª)

è¿™äº›äº§å“å¯èƒ½ä¸çŸ¥è¯†åº“ä¸­çš„äº§å“ç›¸å…³ï¼Œéœ€è¦äººå·¥ç¡®è®¤ï¼š

"""
        for i, product in enumerate(classification['ambiguous'][:20], 1):
            report += f"""### {i}. {product['name']}

- å¯èƒ½åŒ¹é…: {product['possible_match']}
- æåŠæ¬¡æ•°: {product['twitter_data'].get('mention_count', 0)} æ¬¡

---

"""

    # ä¿å­˜æŠ¥å‘Š
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"   âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
    return output_file


def main(twitter_analysis_file, update_db=True):
    """ä¸»æµç¨‹"""

    print("=" * 80)
    print("ğŸš€ Product Knowledge é›†æˆæµç¨‹ v2")
    print("=" * 80)

    # 1. åŠ è½½æ•°æ®
    knowledge_db = load_product_knowledge()
    twitter_analysis = load_twitter_analysis(twitter_analysis_file)

    # 2. åˆ†ç±»äº§å“ï¼ˆå¤„ç†æ‰€æœ‰äº§å“ï¼‰
    classification = classify_products(
        twitter_analysis.get('products', {}),
        knowledge_db
    )

    # 3. æ›´æ–°æ•°æ®åº“
    new_version = None
    if update_db and classification['new_products']:
        new_version = update_knowledge_db(classification['new_products'])

    # 4. ç”ŸæˆæŠ¥å‘Š
    output_file = twitter_analysis_file.replace('analysis_summary.json', 'enhanced_report_v2.md')
    report_path = generate_enhanced_report(twitter_analysis, classification, output_file)

    # 5. ä¿å­˜åˆ†ç±»ç»“æœ
    classification_file = twitter_analysis_file.replace('analysis_summary.json', 'product_classification_v2.json')
    with open(classification_file, 'w', encoding='utf-8') as f:
        json.dump(classification, f, indent=2, ensure_ascii=False)

    print("\n" + "=" * 80)
    print("âœ… é›†æˆå®Œæˆ!")
    print("=" * 80)
    print(f"ğŸ“Š æ–°äº§å“: {len(classification['new_products'])}")
    print(f"ğŸ“¦ å·²æœ‰äº§å“: {len(classification['existing_products'])}")
    print(f"â“ æ¨¡ç³ŠåŒ¹é…: {len(classification['ambiguous'])}")
    if new_version:
        print(f"ğŸ’¾ æ•°æ®åº“ç‰ˆæœ¬: {new_version}")
    print(f"ğŸ“„ å¢å¼ºæŠ¥å‘Š: {report_path}")
    print(f"ğŸ“„ åˆ†ç±»ç»“æœ: {classification_file}")
    print("=" * 80)

    return {
        'classification': classification,
        'new_version': new_version,
        'report_path': report_path
    }


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='é›†æˆ Twitter åˆ†æå’Œ Product Knowledge v2')
    parser.add_argument('analysis_file', help='Twitter åˆ†æç»“æœæ–‡ä»¶ (analysis_summary.json)')
    parser.add_argument('--no-update-db', action='store_true', help='ä¸æ›´æ–°æ•°æ®åº“')

    args = parser.parse_args()

    result = main(
        twitter_analysis_file=args.analysis_file,
        update_db=not args.no_update_db
    )
