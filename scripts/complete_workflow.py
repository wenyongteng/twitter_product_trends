#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Twitter äº§å“è¶‹åŠ¿åˆ†æ - å®Œæ•´å·¥ä½œæµ
æ­£ç¡®çš„é¡ºåºï¼šé‡‡é›† â†’ Product Knowledge å¤„ç† â†’ ç»¼åˆæŠ¥å‘Šç”Ÿæˆ
"""

import json
import subprocess
import sys
from pathlib import Path
from datetime import datetime


def step1_collect_twitter_data(days=7, kol_count=300):
    """
    Step 1: é‡‡é›† Twitter åŸå§‹æ•°æ®
    ä½¿ç”¨ç°æœ‰çš„ collect_data.py
    """
    print("\n" + "=" * 80)
    print("Step 1/4: ğŸ“± é‡‡é›† Twitter åŸå§‹æ•°æ®")
    print("=" * 80)

    twitter_monitor_path = Path("/Users/wenyongteng/twitter hot news/weekly_monitor")
    collect_script = twitter_monitor_path / "collect_data.py"

    print(f"å‚æ•°: days={days}, kol_count={kol_count}")
    print(f"è„šæœ¬: {collect_script}")

    # æ‰§è¡Œé‡‡é›†
    cmd = [sys.executable, str(collect_script), '--days', str(days), '--kol-count', str(kol_count)]

    print("å¼€å§‹é‡‡é›†...")
    result = subprocess.run(cmd, cwd=str(twitter_monitor_path), capture_output=True, text=True)

    if result.returncode != 0:
        print(f"âŒ é‡‡é›†å¤±è´¥: {result.stderr}")
        return None

    print(result.stdout)

    # æ‰¾åˆ°æœ€æ–°çš„æ•°æ®æ–‡ä»¶
    reports_dir = twitter_monitor_path / "weekly_reports"
    latest_week = sorted(reports_dir.glob("week_*"), key=lambda x: x.stat().st_mtime, reverse=True)[0]
    raw_data_file = latest_week / "raw_data.json"

    print(f"âœ… Step 1 å®Œæˆ: {raw_data_file}")

    return {
        'raw_data_file': str(raw_data_file),
        'week_dir': str(latest_week)
    }


def step2_product_knowledge_processing(raw_data_file):
    """
    Step 2: Product Knowledge å¤„ç†
    æå–äº§å“ + åŒ¹é…ç°æœ‰æ•°æ®åº“ + ç”Ÿæˆäº§å“-æ¨æ–‡æ˜ å°„

    è¿™ä¸€æ­¥çš„å…³é”®è¾“å‡ºï¼š
    - æ¯ä¸ªäº§å“å¯¹åº”çš„æ¨æ–‡åˆ—è¡¨ï¼ˆproduct_tweets_map.jsonï¼‰
    - æ–°äº§å“åˆ—è¡¨
    - å·²æœ‰äº§å“åˆ—è¡¨
    """
    print("\n" + "=" * 80)
    print("Step 2/4: ğŸ” Product Knowledge å¤„ç†")
    print("=" * 80)

    print(f"è¾“å…¥: {raw_data_file}")

    # è°ƒç”¨ç°æœ‰çš„ analyze_tweets.py (å®ƒå·²ç»æœ‰äº§å“æå–é€»è¾‘)
    twitter_monitor_path = Path("/Users/wenyongteng/twitter hot news/weekly_monitor")
    analyze_script = twitter_monitor_path / "analyze_tweets.py"

    cmd = [sys.executable, str(analyze_script), raw_data_file]

    print("åˆ†ææ¨æ–‡ä¸­...")
    result = subprocess.run(cmd, cwd=str(twitter_monitor_path), capture_output=True, text=True)

    if result.returncode != 0:
        print(f"âš ï¸  åˆ†æå‡ºé”™: {result.stderr}")
        # ç»§ç»­æ‰§è¡Œï¼Œå› ä¸ºå¯èƒ½åªæ˜¯è­¦å‘Š

    print(result.stdout)

    # åˆ†æç»“æœæ–‡ä»¶
    analysis_file = raw_data_file.replace('raw_data.json', 'analysis_summary.json')

    # è¿è¡Œ Product Knowledge é›†æˆ
    pk_integrate_script = Path(__file__).parent / "integrate_product_knowledge.py"

    cmd = [sys.executable, str(pk_integrate_script), analysis_file]

    print("\næ•´åˆ Product Knowledge...")
    result = subprocess.run(cmd, capture_output=True, text=True)

    if result.returncode != 0:
        print(f"âŒ é›†æˆå¤±è´¥: {result.stderr}")
        return None

    print(result.stdout)

    # è¯»å–åˆ†ç±»ç»“æœ
    classification_file = analysis_file.replace('analysis_summary.json', 'product_classification.json')

    with open(classification_file, 'r', encoding='utf-8') as f:
        classification = json.load(f)

    print(f"âœ… Step 2 å®Œæˆ:")
    print(f"   - æ–°äº§å“: {len(classification['new_products'])}")
    print(f"   - å·²æœ‰äº§å“: {len(classification['existing_products'])}")

    return {
        'analysis_file': analysis_file,
        'classification_file': classification_file,
        'classification': classification
    }


def step3_generate_comprehensive_report(raw_data_file, analysis_file, classification_file):
    """
    Step 3: ç”Ÿæˆç»¼åˆæŠ¥å‘Š

    åˆ†ä¸¤éƒ¨åˆ†ï¼š
    A) äº§å“åˆ†æ - ä½¿ç”¨ Product Knowledge å¤„ç†åçš„æ•°æ®
    B) è¶‹åŠ¿å’Œå°äº‹åˆ†æ - ä½¿ç”¨å…¨éƒ¨åŸå§‹æ¨æ–‡
    """
    print("\n" + "=" * 80)
    print("Step 3/4: ğŸ“ ç”Ÿæˆç»¼åˆæŠ¥å‘Š")
    print("=" * 80)

    # åŠ è½½æ•°æ®
    with open(raw_data_file, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)

    with open(analysis_file, 'r', encoding='utf-8') as f:
        analysis = json.load(f)

    with open(classification_file, 'r', encoding='utf-8') as f:
        classification = json.load(f)

    tweets = raw_data.get('tweets', [])
    metadata = raw_data.get('metadata', {})

    print(f"æ•°æ®åŠ è½½å®Œæˆ:")
    print(f"  - åŸå§‹æ¨æ–‡: {len(tweets)}")
    print(f"  - åˆ†æäº§å“: {len(analysis.get('products', {}))}")
    print(f"  - æ–°äº§å“: {len(classification['new_products'])}")
    print(f"  - å·²æœ‰äº§å“: {len(classification['existing_products'])}")

    # ç”ŸæˆæŠ¥å‘Š
    report_file = raw_data_file.replace('raw_data.json', 'comprehensive_report.md')

    report = generate_report_content(
        tweets=tweets,
        metadata=metadata,
        analysis=analysis,
        classification=classification
    )

    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"âœ… Step 3 å®Œæˆ: {report_file}")

    return report_file


def generate_report_content(tweets, metadata, analysis, classification):
    """ç”ŸæˆæŠ¥å‘Šå†…å®¹"""

    date_range = metadata.get('date_range', {})

    report = f"""# Twitter äº§å“è¶‹åŠ¿åˆ†ææŠ¥å‘Š
## {date_range.get('start', 'N/A')} è‡³ {date_range.get('end', 'N/A')}

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**æ•°æ®æ¥æº**: Twitter Monitor + Product Knowledge é›†æˆ

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

**æ•°æ®æ¦‚è§ˆ**
- ç›‘æ§ KOL: {metadata.get('kol_count', 'N/A')} ä¸ª
- åˆ†ææ¨æ–‡: {len(tweets):,} æ¡
- è¯†åˆ«äº§å“: {len(analysis.get('products', {}))} ä¸ª
  - **ğŸ†• æ–°äº§å“**: {len(classification['new_products'])} ä¸ª
  - **ğŸ“¦ å·²æœ‰äº§å“**: {len(classification['existing_products'])} ä¸ª

**æ ¸å¿ƒå‘ç°**
1. æœ¬å‘¨å‘ç° {len(classification['new_products'])} ä¸ªæ–°äº§å“
2. {len(classification['existing_products'])} ä¸ªå·²æœ‰äº§å“ç»§ç»­æ´»è·ƒ
3. [å…¶ä»–å…³é”®è¶‹åŠ¿...]

---

## ç¬¬ä¸€éƒ¨åˆ†: äº§å“åˆ†æ (åŸºäº Product Knowledge)

### ğŸ†• æ–°äº§å“å‘ç° ({len(classification['new_products'])} ä¸ª)

"""

    # A) æ–°äº§å“è¯¦æƒ… (ä½¿ç”¨ Product Knowledge æ•°æ®)
    for i, product in enumerate(classification['new_products'][:20], 1):
        name = product['name']
        twitter_data = product['twitter_data']

        report += f"""#### {i}. {name}

**åŸºæœ¬ä¿¡æ¯**
- æåŠæ¬¡æ•°: {twitter_data.get('mention_count', 0)} æ¬¡
- æ€»äº’åŠ¨æ•°: {twitter_data.get('total_engagement', 0)}
- è®¨è®ºçƒ­åº¦: {'â­' * min(5, twitter_data.get('mention_count', 0) // 5 + 1)}

**Top KOLs**
{chr(10).join(f"- @{kol}" for kol in twitter_data.get('top_kols', [])[:3]) if twitter_data.get('top_kols') else '- (æ— )'}

**ç¤ºä¾‹æ¨æ–‡**
```
{twitter_data.get('sample_tweets', [{}])[0].get('text', 'N/A')[:200] if twitter_data.get('sample_tweets') else 'N/A'}
```

---

"""

    # B) å·²æœ‰äº§å“çƒ­åº¦ (ä½¿ç”¨ Product Knowledge æ•°æ®)
    report += f"""

### ğŸ“¦ çƒ­é—¨å·²æœ‰äº§å“ Top 20

"""

    for i, product in enumerate(classification['existing_products'][:20], 1):
        name = product['kb_canonical_name']
        twitter_data = product['twitter_data']
        kb_data = product['knowledge_data']

        report += f"""#### {i}. {name}

**äº§å“ä¿¡æ¯** (æ¥è‡ªçŸ¥è¯†åº“)
- å…¬å¸: {kb_data.get('company', 'Unknown')}
- ç±»åˆ«: {kb_data.get('category', 'Unknown')}

**æœ¬å‘¨åŠ¨æ€**
- æåŠæ¬¡æ•°: {twitter_data.get('mention_count', 0)} æ¬¡
- æ€»äº’åŠ¨æ•°: {twitter_data.get('total_engagement', 0)}

---

"""

    # C) è¶‹åŠ¿åˆ†æ (ä½¿ç”¨å…¨éƒ¨åŸå§‹æ¨æ–‡)
    report += f"""

---

## ç¬¬äºŒéƒ¨åˆ†: è¶‹åŠ¿ä¸å°äº‹åˆ†æ (åŸºäºå…¨éƒ¨æ¨æ–‡)

### ğŸ“ˆ å®è§‚è¶‹åŠ¿

**çƒ­é—¨è¯é¢˜**
"""

    top_topics = analysis.get('summary', {}).get('top_topics', {})
    for topic, count in sorted(top_topics.items(), key=lambda x: x[1], reverse=True)[:10]:
        report += f"- {topic}: {count} æ¬¡æåŠ\n"

    report += f"""

### ğŸ’ å€¼å¾—æ³¨æ„çš„å°äº‹

[TODO: åŸºäºå…¨éƒ¨æ¨æ–‡çš„æ·±åº¦åˆ†æï¼Œè¯†åˆ«å°‘æ•°äººæåˆ°ä½†é‡è¦çš„å†…å®¹]

---

## ğŸ“Š æ•°æ®é™„å½•

### Top KOL æ´»è·ƒåº¦
"""

    top_kols = analysis.get('top_kols', {})
    for kol, count in sorted(top_kols.items(), key=lambda x: x[1], reverse=True)[:10]:
        report += f"- @{kol}: {count} æ¡æ¨æ–‡\n"

    report += f"""

---

**æŠ¥å‘Šç”Ÿæˆ**: Claude Code - Twitter Product Trends Analyzer
**æ•°æ®æº**: Twitter API + Product Knowledge Database
"""

    return report


def main(days=7, kol_count=300, use_existing_data=False):
    """å®Œæ•´å·¥ä½œæµä¸»å‡½æ•°"""

    print("=" * 80)
    print("ğŸš€ Twitter äº§å“è¶‹åŠ¿åˆ†æ - å®Œæ•´å·¥ä½œæµ")
    print("=" * 80)
    print(f"å‚æ•°: days={days}, kol_count={kol_count}")
    print(f"ä½¿ç”¨å·²æœ‰æ•°æ®: {use_existing_data}")
    print("=" * 80)

    start_time = datetime.now()

    try:
        # Step 1: é‡‡é›†æ•°æ®
        if use_existing_data:
            print("\nğŸ§ª ä½¿ç”¨å·²æœ‰æ•°æ®æ¨¡å¼")
            twitter_monitor_path = Path("/Users/wenyongteng/twitter hot news/weekly_monitor")
            reports_dir = twitter_monitor_path / "weekly_reports"
            latest_week = sorted(reports_dir.glob("week_*"), key=lambda x: x.stat().st_mtime, reverse=True)[0]

            step1_result = {
                'raw_data_file': str(latest_week / "raw_data.json"),
                'week_dir': str(latest_week)
            }
            print(f"ä½¿ç”¨: {step1_result['raw_data_file']}")
        else:
            step1_result = step1_collect_twitter_data(days, kol_count)
            if not step1_result:
                return False

        # Step 2: Product Knowledge å¤„ç†
        step2_result = step2_product_knowledge_processing(step1_result['raw_data_file'])
        if not step2_result:
            return False

        # Step 3: ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report_file = step3_generate_comprehensive_report(
            raw_data_file=step1_result['raw_data_file'],
            analysis_file=step2_result['analysis_file'],
            classification_file=step2_result['classification_file']
        )

        # æ€»ç»“
        elapsed = (datetime.now() - start_time).total_seconds()

        print("\n" + "=" * 80)
        print("âœ… å®Œæ•´æµç¨‹æ‰§è¡ŒæˆåŠŸ!")
        print("=" * 80)
        print(f"â±ï¸  æ€»è€—æ—¶: {elapsed:.1f} ç§’ ({elapsed/60:.1f} åˆ†é’Ÿ)")
        print(f"ğŸ“ å·¥ä½œç›®å½•: {step1_result['week_dir']}")
        print(f"ğŸ“„ ç»¼åˆæŠ¥å‘Š: {report_file}")
        print("=" * 80)

        return True

    except Exception as e:
        print(f"\nâŒ æµç¨‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='Twitter äº§å“è¶‹åŠ¿åˆ†æ - å®Œæ•´å·¥ä½œæµ')
    parser.add_argument('--days', type=int, default=7, help='æ—¶é—´èŒƒå›´(å¤©)')
    parser.add_argument('--kol-count', type=int, default=300, help='KOLæ•°é‡')
    parser.add_argument('--use-existing', action='store_true', help='ä½¿ç”¨å·²æœ‰æ•°æ®')

    args = parser.parse_args()

    success = main(
        days=args.days,
        kol_count=args.kol_count,
        use_existing_data=args.use_existing
    )

    sys.exit(0 if success else 1)
