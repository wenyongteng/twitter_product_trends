#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Product Knowledge é›†æˆè„šæœ¬ v3
ç›´æ¥ä» raw_data.json æå–æ‰€æœ‰äº§å“ï¼ˆä¸é™äº Top 30ï¼‰
"""

import json
import re
from pathlib import Path
from datetime import datetime
from collections import defaultdict, Counter
from typing import Dict, List, Set


# ============ äº§å“æå–é€»è¾‘ (å¤ç”¨ analyze_tweets.py) ============

def extract_products(text: str) -> List[str]:
    """ä»æ–‡æœ¬ä¸­æå–äº§å“å"""
    products = set()

    # äº§å“æå–è§„åˆ™ (å¤ç”¨ analyze_tweets.py çš„é€»è¾‘)
    patterns = [
        # AI æ¨¡å‹å’Œäº§å“
        r'\b(GPT-[0-9o]+(?:\s+(?:mini|turbo|pro|high|minimal|codex))?)\b',
        r'\b(Claude(?:\s+[0-9.]+)?(?:\s+(?:Opus|Sonnet|Haiku))?)\b',
        r'\b(Gemini(?:\s+[0-9.]+)?(?:\s+(?:Pro|Flash|Ultra|Nano\s+Banana))?)\b',
        r'\b(Llama\s+[0-9.]+)\b',
        r'\b(Mistral\s+[0-9.]+)\b',
        r'\b(DeepSeek(?:\s+V[0-9.]+)?(?:\s+(?:Exp|R1))?)\b',
        r'\b(Qwen(?:[0-9.]+)?)\b',

        # å…¬å¸å’Œå¹³å°
        r'\b(OpenAI|Anthropic|Google|Meta|xAI|Microsoft)\b',
        r'\b(ChatGPT|Copilot|Cursor|Midjourney)\b',
        r'\b(GitHub\s+Copilot|VS\s+Code)\b',

        # å·¥å…·å’Œåº”ç”¨
        r'\b(Sora|DALL-E|Stable\s+Diffusion|Seedream)\b',
        r'\b(NotebookLM|Lovable|Replit|Vercel)\b',
        r'\b(Perplexity|Grok)\b',
    ]

    for pattern in patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            if len(match.groups()) > 0:
                products.add(match.group(1) if match.group(1) else match.group(0))
            else:
                products.add(match.group(0))

    return list(products)


def get_sentiment(text: str) -> str:
    """ç®€å•æƒ…æ„Ÿåˆ†æ"""
    positive_words = ['love', 'amazing', 'great', 'awesome', 'excellent', 'fantastic', 'incredible', 'best']
    negative_words = ['hate', 'terrible', 'awful', 'bad', 'poor', 'disappointed', 'worst', 'sucks']

    text_lower = text.lower()
    pos_count = sum(1 for word in positive_words if word in text_lower)
    neg_count = sum(1 for word in negative_words if word in text_lower)

    if pos_count > neg_count:
        return 'positive'
    elif neg_count > pos_count:
        return 'negative'
    else:
        return 'neutral'


# ============ Product Knowledge æ•°æ®åº“æ“ä½œ ============

def load_product_knowledge(pk_version_path: str) -> Dict[str, Dict]:
    """åŠ è½½ Product Knowledge æ•°æ®åº“ (æ”¯æŒ list æ ¼å¼)"""
    products_file = Path(pk_version_path) / "products_list.json"

    if not products_file.exists():
        print(f"âš ï¸  Product Knowledge æ–‡ä»¶ä¸å­˜åœ¨: {products_file}")
        return {}

    with open(products_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # å¤„ç† list æ ¼å¼: {"total_products": N, "products": [{...}, ...]}
    products_dict = {}

    if isinstance(data, dict) and 'products' in data:
        for product in data['products']:
            if isinstance(product, dict) and 'name' in product:
                name = product['name']
                # ä¹Ÿå­˜å‚¨å°å†™ç‰ˆæœ¬ä½œä¸º alias
                products_dict[name] = product
                products_dict[name.lower()] = product

                # å¤„ç†åˆ«å
                if 'aliases' in product:
                    for alias in product.get('aliases', []):
                        products_dict[alias] = product
                        products_dict[alias.lower()] = product

    # è®¡ç®—å”¯ä¸€äº§å“æ•°é‡ (é€šè¿‡ id)
    unique_products = set()
    for product in products_dict.values():
        if isinstance(product, dict) and 'id' in product:
            unique_products.add(product['id'])

    print(f"âœ… åŠ è½½äº† {len(unique_products)} ä¸ªäº§å“ (å«åˆ«åå…± {len(products_dict)} æ¡è®°å½•)")

    return products_dict


def normalize_product_name(name: str) -> str:
    """æ ‡å‡†åŒ–äº§å“åï¼ˆå¤§å°å†™æ•æ„Ÿï¼Œä¿ç•™ç‰ˆæœ¬å·å·®å¼‚ï¼‰"""
    # å»é™¤å¤šä½™ç©ºæ ¼
    name = ' '.join(name.split())
    name = name.strip()

    # å¤§å°å†™å½’ä¸€åŒ–è§„åˆ™ï¼ˆåªå¤„ç†å®Œå…¨ä¸€è‡´çš„åç§°ï¼‰
    # ä¾‹å¦‚: "GOOGLE" -> "Google", "google" -> "Google"
    # ä½†ä¿ç•™: "Gemini 3" å’Œ "gemini 3 pro" ä½œä¸ºä¸åŒç‰ˆæœ¬

    # æ£€æŸ¥æ˜¯å¦åªæ˜¯å¤§å°å†™ä¸åŒï¼ˆæ²¡æœ‰å…¶ä»–å·®å¼‚ï¼‰
    lower_name = name.lower()

    # å…¬å¸åç§°å¤§å°å†™å½’ä¸€åŒ–
    company_mappings = {
        'google': 'Google',
        'microsoft': 'Microsoft',
        'meta': 'Meta',
        'openai': 'OpenAI',
        'anthropic': 'Anthropic'
    }

    if lower_name in company_mappings:
        return company_mappings[lower_name]

    # äº§å“åç§°å¤§å°å†™å½’ä¸€åŒ–ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
    product_mappings = {
        'claude': 'Claude',
        'chatgpt': 'ChatGPT',
        'gemini': 'Gemini',
        'sora': 'Sora',
        'copilot': 'Copilot',
        'cursor': 'Cursor',
        'grok': 'Grok',
        'qwen': 'Qwen',
        'vercel': 'Vercel'
    }

    if lower_name in product_mappings:
        return product_mappings[lower_name]

    # å…¶ä»–æƒ…å†µä¿æŒåŸæ ·ï¼ˆä¿ç•™ç‰ˆæœ¬å·å·®å¼‚ï¼‰
    return name


def match_product_to_knowledge(product_name: str, pk_dict: Dict) -> tuple:
    """
    åŒ¹é…äº§å“åˆ°çŸ¥è¯†åº“
    è¿”å›: (åŒ¹é…ç±»å‹, è§„èŒƒåç§°, çŸ¥è¯†æ•°æ®)
    åŒ¹é…ç±»å‹: 'exact' | 'fuzzy' | 'new'
    """
    normalized = normalize_product_name(product_name)

    # ç²¾ç¡®åŒ¹é…
    if normalized in pk_dict:
        kb_product = pk_dict[normalized]
        return ('exact', kb_product.get('name', normalized), kb_product)

    # å¤§å°å†™ä¸æ•æ„ŸåŒ¹é…
    if normalized.lower() in pk_dict:
        kb_product = pk_dict[normalized.lower()]
        return ('exact', kb_product.get('name', normalized), kb_product)

    # æ¨¡ç³ŠåŒ¹é… (ç®€åŒ–ç‰ˆ)
    for kb_name, kb_product in pk_dict.items():
        if kb_name.lower() == normalized.lower():
            return ('fuzzy', kb_product.get('name', kb_name), kb_product)

    # æ²¡æ‰¾åˆ° -> æ–°äº§å“
    return ('new', normalized, None)


# ============ ä¸»å¤„ç†æµç¨‹ ============

def extract_all_products_from_raw_data(raw_data_file: str) -> Dict:
    """ä» raw_data.json æå–æ‰€æœ‰äº§å“ (ä¸é™äº Top 30)"""

    print(f"\nğŸ“‚ è¯»å–åŸå§‹æ¨æ–‡æ•°æ®: {raw_data_file}")

    with open(raw_data_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    tweets = data.get('tweets', [])
    print(f"   - æ¨æ–‡æ€»æ•°: {len(tweets)}")

    # äº§å“ç»Ÿè®¡
    product_mentions = defaultdict(list)  # product -> [tweets]
    sentiment_stats = defaultdict(Counter)  # product -> {positive: N, neutral: M, ...}
    kol_mentions = defaultdict(set)  # product -> {kol1, kol2, ...}

    print(f"\nğŸ” æå–æ‰€æœ‰äº§å“...")

    for i, tweet in enumerate(tweets, 1):
        if i % 500 == 0:
            print(f"   å¤„ç†è¿›åº¦: {i}/{len(tweets)}")

        text = tweet.get('text', '')
        author = tweet.get('author', {})
        kol = author.get('username', 'unknown') if isinstance(author, dict) else str(author)

        # æå–äº§å“
        products = extract_products(text)

        for product in products:
            # æ ‡å‡†åŒ–äº§å“å
            product = normalize_product_name(product)

            # è®°å½•æåŠ
            product_mentions[product].append({
                'text': text,
                'kol': kol,
                'rank': tweet.get('rank', 0),
                'followers': tweet.get('followers', 0),
                'likes': tweet.get('likes', 0),
                'retweets': tweet.get('retweets', 0),
                'created_at': tweet.get('created_at', ''),
                'sentiment': get_sentiment(text),
                'is_new': 'launch' in text.lower() or 'new' in text.lower()
            })

            # ç»Ÿè®¡
            sentiment_stats[product][get_sentiment(text)] += 1
            if kol:  # ç¡®ä¿ kol ä¸ä¸ºç©º
                kol_mentions[product].add(kol)

    print(f"\nâœ… æå–å®Œæˆ!")
    print(f"   - è¯†åˆ«äº§å“: {len(product_mentions)} ä¸ª")

    # æ„é€  twitter_products æ•°æ®ç»“æ„
    twitter_products = {}

    for product, mentions in product_mentions.items():
        # æŒ‰äº’åŠ¨æ•°æ’åº
        sorted_mentions = sorted(
            mentions,
            key=lambda x: x['likes'] + x['retweets'],
            reverse=True
        )

        # Top KOLs (æŒ‰æåŠæ¬¡æ•°)
        kol_counts = Counter([m['kol'] for m in mentions])
        top_kols = [kol for kol, _ in kol_counts.most_common(5)]

        twitter_products[product] = {
            'mention_count': len(mentions),
            'top_kols': top_kols,
            'sentiment': dict(sentiment_stats[product]),
            'total_engagement': sum(m['likes'] + m['retweets'] for m in mentions),
            'sample_tweets': sorted_mentions[:3]  # Top 3 æ¨æ–‡
        }

    return twitter_products


def is_company_entity(product_name: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºå…¬å¸å®ä½“ï¼ˆè€Œéäº§å“ï¼‰"""
    # å®šä¹‰å…¬å¸å®ä½“åˆ—è¡¨
    companies = {
        'Google', 'Microsoft', 'Meta', 'OpenAI', 'Anthropic',
        'xAI', 'NVIDIA', 'Apple', 'Amazon', 'Tesla',
        'DeepMind', 'Hugging Face', 'Stability AI'
    }

    return product_name in companies


def classify_products(twitter_products: Dict, pk_dict: Dict) -> Dict:
    """åˆ†ç±»äº§å“: æ–°äº§å“ vs å·²æœ‰äº§å“ vs å…¬å¸å®ä½“"""

    new_products = []
    existing_products = []
    ambiguous = []
    companies = []  # å…¬å¸å®ä½“å•ç‹¬åˆ†ç±»

    print(f"\nğŸ” åˆ†ç±»äº§å“...")
    print(f"   - Twitter äº§å“æ•°: {len(twitter_products)}")

    # è®¡ç®—å”¯ä¸€äº§å“æ•°é‡
    unique_kb_products = set()
    for product in pk_dict.values():
        if isinstance(product, dict) and 'id' in product:
            unique_kb_products.add(product['id'])

    print(f"   - çŸ¥è¯†åº“äº§å“æ•°: {len(unique_kb_products)}")

    for product_name, twitter_data in twitter_products.items():
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºå…¬å¸å®ä½“
        if is_company_entity(product_name):
            companies.append({
                'name': product_name,
                'twitter_data': twitter_data,
                'type': 'company'
            })
            continue

        match_type, canonical_name, kb_data = match_product_to_knowledge(product_name, pk_dict)

        if match_type == 'exact':
            # å·²æœ‰äº§å“
            existing_products.append({
                'name': product_name,
                'twitter_data': twitter_data,
                'knowledge_data': kb_data,
                'kb_canonical_name': canonical_name
            })

        elif match_type == 'fuzzy':
            # æ¨¡ç³ŠåŒ¹é… - éœ€è¦äººå·¥ç¡®è®¤
            ambiguous.append({
                'name': product_name,
                'twitter_data': twitter_data,
                'possible_match': canonical_name,
                'knowledge_data': kb_data
            })

        else:  # match_type == 'new'
            # æ–°äº§å“
            new_products.append({
                'name': product_name,
                'twitter_data': twitter_data
            })

    # æŒ‰æåŠæ¬¡æ•°æ’åº
    new_products.sort(key=lambda x: x['twitter_data']['mention_count'], reverse=True)
    existing_products.sort(key=lambda x: x['twitter_data']['mention_count'], reverse=True)
    companies.sort(key=lambda x: x['twitter_data']['mention_count'], reverse=True)

    print(f"\nğŸ“Š åˆ†ç±»ç»“æœ:")
    print(f"   - æ–°äº§å“: {len(new_products)} ä¸ª")
    print(f"   - å·²æœ‰äº§å“: {len(existing_products)} ä¸ª")
    print(f"   - å…¬å¸å®ä½“: {len(companies)} ä¸ª")
    print(f"   - æ¨¡ç³ŠåŒ¹é…: {len(ambiguous)} ä¸ª")

    return {
        'new_products': new_products,
        'existing_products': existing_products,
        'companies': companies,
        'ambiguous': ambiguous
    }


def generate_enhanced_report(classification: Dict, output_file: str, date_range: Dict):
    """ç”Ÿæˆå¢å¼ºç‰ˆæŠ¥å‘Š"""

    new_products = classification['new_products']
    existing_products = classification['existing_products']
    companies = classification.get('companies', [])
    ambiguous = classification['ambiguous']

    total_items = len(new_products) + len(existing_products) + len(companies) + len(ambiguous)

    report = f"""# Twitter Product Trends Report (Enhanced)
## {date_range.get('start', 'N/A')} è‡³ {date_range.get('end', 'N/A')}

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**æ•°æ®æ¥æº**: Twitter Monitor + Product Knowledge

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

**æ•°æ®æ¦‚è§ˆ**
- æ€»æ¨æ–‡æ•°: {date_range.get('total_tweets', 'N/A')}
- è¯†åˆ«å®ä½“: {total_items} ä¸ª
  - **ğŸ†• æ–°äº§å“**: {len(new_products)} ä¸ª
  - **ğŸ“¦ å·²æœ‰äº§å“ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰**: {len(existing_products)} ä¸ª
  - **ğŸ¢ å…¬å¸å®ä½“**: {len(companies)} ä¸ª
  - **â“ æ¨¡ç³ŠåŒ¹é…**: {len(ambiguous)} ä¸ª

---

## ğŸ†• æ–°äº§å“å‘ç° ({len(new_products)}ä¸ª)

è¿™äº›äº§å“åœ¨ Product Knowledge æ•°æ®åº“ä¸­ä¸å­˜åœ¨ï¼š

"""

    for i, product in enumerate(new_products, 1):
        name = product['name']
        twitter_data = product['twitter_data']

        report += f"""### {i}. {name}

- æåŠæ¬¡æ•°: {twitter_data.get('mention_count', 0)} æ¬¡
- æ€»äº’åŠ¨æ•°: {twitter_data.get('total_engagement', 0)}
- Top KOLs: {', '.join(['@' + k for k in twitter_data.get('top_kols', [])[:3]])}

---

"""

    report += f"""

## ğŸ“¦ å·²æœ‰äº§å“ ({len(existing_products)}ä¸ª)

è¿™äº›äº§å“åœ¨ Product Knowledge æ•°æ®åº“ä¸­å·²å­˜åœ¨ï¼š

"""

    for i, product in enumerate(existing_products, 1):
        name = product['kb_canonical_name']
        twitter_data = product['twitter_data']
        kb_data = product['knowledge_data']

        report += f"""### {i}. {name}

**çŸ¥è¯†åº“ä¿¡æ¯**
- å…¬å¸: {kb_data.get('company', 'Unknown')}
- å†å²æåŠ: {kb_data.get('mention_count', 0)} æ¬¡

**æœ¬å‘¨æ•°æ®**
- æåŠæ¬¡æ•°: {twitter_data.get('mention_count', 0)} æ¬¡
- æ€»äº’åŠ¨æ•°: {twitter_data.get('total_engagement', 0)}

---

"""

    if companies:
        report += f"""

## ğŸ¢ å…¬å¸å®ä½“ ({len(companies)}ä¸ª)

è¿™äº›æ˜¯å…¬å¸/ç»„ç»‡åç§°ï¼ˆéäº§å“ï¼‰ï¼š

"""
        for i, company in enumerate(companies, 1):
            name = company['name']
            twitter_data = company['twitter_data']

            report += f"""### {i}. {name}

- æåŠæ¬¡æ•°: {twitter_data.get('mention_count', 0)} æ¬¡
- æ€»äº’åŠ¨æ•°: {twitter_data.get('total_engagement', 0)}
- Top KOLs: {', '.join(['@' + k for k in twitter_data.get('top_kols', [])[:3]])}

---

"""

    if ambiguous:
        report += f"""

## â“ æ¨¡ç³ŠåŒ¹é… ({len(ambiguous)}ä¸ª)

è¿™äº›äº§å“å¯èƒ½ä¸çŸ¥è¯†åº“ä¸­çš„äº§å“ç›¸å…³ï¼Œéœ€è¦äººå·¥ç¡®è®¤ï¼š

"""
        for i, product in enumerate(ambiguous, 1):
            name = product['name']
            possible_match = product['possible_match']
            twitter_data = product['twitter_data']

            report += f"""### {i}. {name}

- å¯èƒ½åŒ¹é…: {possible_match}
- æåŠæ¬¡æ•°: {twitter_data.get('mention_count', 0)} æ¬¡

---

"""

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"\nâœ… å¢å¼ºç‰ˆæŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")


def update_product_knowledge(new_products: List[Dict], pk_version_path: str) -> str:
    """æ›´æ–° Product Knowledge æ•°æ®åº“"""

    if not new_products:
        print("\nâš ï¸  æ²¡æœ‰æ–°äº§å“éœ€è¦æ·»åŠ åˆ°æ•°æ®åº“")
        return pk_version_path

    print(f"\nğŸ“ å‡†å¤‡æ›´æ–° Product Knowledge æ•°æ®åº“...")
    print(f"   - æ–°äº§å“æ•°é‡: {len(new_products)}")

    # åŠ è½½å½“å‰ç‰ˆæœ¬
    current_version = Path(pk_version_path)
    products_file = current_version / "products_list.json"

    with open(products_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    original_count = data['total_products']
    products_list = data['products']

    # è®¡ç®—æ–°çš„ ID
    max_id = max([p.get('id', 0) for p in products_list], default=0)

    # æ·»åŠ æ–°äº§å“
    for i, new_product in enumerate(new_products, 1):
        name = new_product['name']
        twitter_data = new_product['twitter_data']

        # æ‰¾åˆ°é¦–æ¬¡æåŠçš„æ¨æ–‡
        first_tweet = twitter_data.get('sample_tweets', [{}])[-1]  # æœ€åä¸€æ¡é€šå¸¸æœ€æ—©

        products_list.append({
            'id': max_id + i,
            'name': name,
            'company': None,  # éœ€è¦äººå·¥è¡¥å……
            'versions': [],
            'mention_count': twitter_data.get('mention_count', 0),
            'first_mention_time': first_tweet.get('created_at', datetime.now().isoformat()),
            'first_mention_tweet_id': None,  # éœ€è¦ä»åŸå§‹æ•°æ®è·å–
            'confidence': 0.7  # é»˜è®¤ç½®ä¿¡åº¦
        })

    # åˆ›å»ºæ–°ç‰ˆæœ¬
    today = datetime.now().strftime('%Y%m%d')
    new_version_name = f"v2_twitter_{today}"
    new_version_path = current_version.parent / new_version_name

    new_version_path.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜æ›´æ–°åçš„äº§å“åˆ—è¡¨
    new_data = {
        'total_products': len(products_list),
        'products': products_list
    }

    new_products_file = new_version_path / "products_list.json"
    with open(new_products_file, 'w', encoding='utf-8') as f:
        json.dump(new_data, f, ensure_ascii=False, indent=2)

    # ä¿å­˜ç‰ˆæœ¬å…ƒæ•°æ®
    metadata = {
        'version': new_version_name,
        'created_at': datetime.now().isoformat(),
        'based_on': current_version.name,
        'type': 'twitter_update',
        'changes': {
            'new_products_added': len(new_products),
            'original_product_count': original_count,
            'new_product_count': len(products_list)
        },
        'new_products_list': [p['name'] for p in new_products]
    }

    metadata_file = new_version_path / "metadata.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… Product Knowledge å·²æ›´æ–°!")
    print(f"   - åŸç‰ˆæœ¬: {current_version.name} ({original_count} ä¸ªäº§å“)")
    print(f"   - æ–°ç‰ˆæœ¬: {new_version_name} ({len(products_list)} ä¸ªäº§å“)")
    print(f"   - æ–°å¢äº§å“: {len(new_products)} ä¸ª")
    print(f"   - æ–°ç‰ˆæœ¬è·¯å¾„: {new_version_path}")

    return str(new_version_path)


def main(raw_data_file: str):
    """ä¸»æµç¨‹"""

    print("=" * 80)
    print("ğŸš€ Product Knowledge Integration v3 (å¤„ç†æ‰€æœ‰äº§å“)")
    print("=" * 80)

    # 1. ä» raw_data.json æå–æ‰€æœ‰äº§å“
    twitter_products = extract_all_products_from_raw_data(raw_data_file)

    # 2. åŠ è½½ Product Knowledge
    config_file = Path(__file__).parent.parent / "config" / "integration_config.json"
    with open(config_file, 'r', encoding='utf-8') as f:
        config = json.load(f)

    pk_project_path = Path(config['product_knowledge']['project_path'])
    pk_current_version = config['product_knowledge']['current_version']
    pk_version_path = pk_project_path / "versions" / pk_current_version

    pk_dict = load_product_knowledge(str(pk_version_path))

    # 3. åˆ†ç±»äº§å“
    classification = classify_products(twitter_products, pk_dict)

    # 4. ç”ŸæˆæŠ¥å‘Š
    week_dir = Path(raw_data_file).parent
    output_file = week_dir / "enhanced_report_v3.md"

    with open(raw_data_file, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)

    date_range = raw_data.get('metadata', {}).get('date_range', {})
    date_range['total_tweets'] = len(raw_data.get('tweets', []))

    generate_enhanced_report(classification, str(output_file), date_range)

    # 5. ä¿å­˜åˆ†ç±»ç»“æœ
    classification_file = week_dir / "product_classification_v3.json"
    with open(classification_file, 'w', encoding='utf-8') as f:
        json.dump(classification, f, ensure_ascii=False, indent=2)

    print(f"âœ… äº§å“åˆ†ç±»å·²ä¿å­˜: {classification_file}")

    # 6. æ›´æ–° Product Knowledge (å¯é€‰)
    if classification['new_products']:
        update_pk = input(f"\nå‘ç° {len(classification['new_products'])} ä¸ªæ–°äº§å“ã€‚æ˜¯å¦æ›´æ–° Product Knowledge? (y/n): ")
        if update_pk.lower() == 'y':
            new_version_path = update_product_knowledge(
                classification['new_products'],
                str(pk_version_path)
            )
            print(f"\nğŸ’¡ æç¤º: è®°å¾—æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„ current_version ä¸º: {Path(new_version_path).name}")

    print("\n" + "=" * 80)
    print("âœ… å®Œæˆ!")
    print("=" * 80)


if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("ç”¨æ³•: python integrate_product_knowledge_v3.py <raw_data.json>")
        sys.exit(1)

    raw_data_file = sys.argv[1]
    main(raw_data_file)
