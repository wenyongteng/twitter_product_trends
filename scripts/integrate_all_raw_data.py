#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Twitter åŸå§‹æ•°æ®é›†æˆå·¥å…·
éå†æ‰€æœ‰å‘¨æŠ¥ç›®å½•ï¼Œæå–æ‰€æœ‰ raw_data.json ä¸­çš„æ¨æ–‡æ•°æ®
"""

import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List
from collections import defaultdict


def find_all_raw_data_files(base_dir: str) -> List[Dict]:
    """æŸ¥æ‰¾æ‰€æœ‰åŒ…å« raw_data.json çš„å‘¨æŠ¥ç›®å½•"""
    reports_dir = Path(base_dir)

    if not reports_dir.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {base_dir}")
        return []

    raw_data_files = []

    for item in reports_dir.iterdir():
        if item.is_dir() and item.name.startswith('week_'):
            raw_data_path = item / 'raw_data.json'

            if raw_data_path.exists():
                file_size = raw_data_path.stat().st_size / (1024 * 1024)  # MB

                report_info = {
                    'directory': str(item),
                    'week_name': item.name,
                    'raw_data_path': str(raw_data_path),
                    'file_size_mb': round(file_size, 2),
                }

                # æå–æ—¥æœŸèŒƒå›´
                if '_to_' in item.name:
                    parts = item.name.replace('week_', '').split('_to_')
                    if len(parts) >= 2:
                        report_info['start_date'] = parts[0]
                        report_info['end_date'] = parts[1].split('_')[0]  # å»æ‰æ¨¡å‹åç¼€

                raw_data_files.append(report_info)

    # æŒ‰æ—¥æœŸæ’åº
    raw_data_files.sort(key=lambda x: x.get('start_date', ''))

    return raw_data_files


def load_raw_tweets(file_path: str) -> List[Dict]:
    """åŠ è½½åŸå§‹æ¨æ–‡æ•°æ®"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # raw_data.json çš„ç»“æ„å¯èƒ½æ˜¯ {'tweets': [...]} æˆ–ç›´æ¥æ˜¯åˆ—è¡¨
        if isinstance(data, dict):
            tweets = data.get('tweets', [])
            metadata = data.get('metadata', {})
        else:
            tweets = data
            metadata = {}

        return tweets, metadata

    except Exception as e:
        print(f"âš ï¸  åŠ è½½å¤±è´¥ {file_path}: {e}")
        return [], {}


def integrate_all_raw_data(raw_data_files: List[Dict]) -> Dict:
    """é›†æˆæ‰€æœ‰åŸå§‹æ¨æ–‡æ•°æ®"""

    integrated_data = {
        'metadata': {
            'integration_date': datetime.now().isoformat(),
            'total_weeks': len(raw_data_files),
            'data_sources': [],
        },
        'all_tweets': [],  # æ‰€æœ‰æ¨æ–‡çš„é›†åˆ
        'tweets_by_week': {},  # æŒ‰å‘¨åˆ†ç»„çš„æ¨æ–‡
        'statistics': {
            'total_tweets': 0,
            'total_kols': 0,
            'date_range': {
                'earliest': None,
                'latest': None,
            },
        },
        'kol_activity': {},  # KOL æ´»è·ƒåº¦ç»Ÿè®¡
        'weekly_summaries': [],  # æ¯å‘¨æ‘˜è¦
    }

    all_kols = set()
    kol_tweets = defaultdict(list)
    kol_stats = defaultdict(lambda: {'tweet_count': 0, 'total_likes': 0, 'total_retweets': 0, 'weeks': []})

    for report in raw_data_files:
        print(f"ğŸ“Š å¤„ç†: {report['week_name']} ({report['file_size_mb']} MB)")

        # åŠ è½½æ¨æ–‡æ•°æ®
        tweets, metadata = load_raw_tweets(report['raw_data_path'])

        if not tweets:
            print(f"  âš ï¸  æ²¡æœ‰æ¨æ–‡æ•°æ®")
            continue

        week_name = report['week_name']

        # è®°å½•æ•°æ®æº
        integrated_data['metadata']['data_sources'].append({
            'week': week_name,
            'date_range': f"{report.get('start_date', 'N/A')} to {report.get('end_date', 'N/A')}",
            'tweet_count': len(tweets),
            'file_size_mb': report['file_size_mb'],
        })

        # ä¸ºæ¯æ¡æ¨æ–‡æ·»åŠ å‘¨ä¿¡æ¯
        week_tweets = []
        for tweet in tweets:
            # æ·»åŠ å…ƒæ•°æ®
            tweet_with_meta = tweet.copy()
            tweet_with_meta['source_week'] = week_name
            tweet_with_meta['source_date_range'] = {
                'start': report.get('start_date'),
                'end': report.get('end_date'),
            }

            week_tweets.append(tweet_with_meta)
            integrated_data['all_tweets'].append(tweet_with_meta)

            # ç»Ÿè®¡ KOL æ´»è·ƒåº¦
            kol_info = tweet.get('kol_info', {})
            if kol_info:
                username = kol_info.get('username', 'unknown')
                all_kols.add(username)
                kol_tweets[username].append(tweet_with_meta)

                # æ›´æ–°ç»Ÿè®¡
                kol_stats[username]['tweet_count'] += 1
                kol_stats[username]['total_likes'] += tweet.get('public_metrics', {}).get('like_count', 0)
                kol_stats[username]['total_retweets'] += tweet.get('public_metrics', {}).get('retweet_count', 0)
                if week_name not in kol_stats[username]['weeks']:
                    kol_stats[username]['weeks'].append(week_name)

                # ä¿å­˜ KOL è¯¦ç»†ä¿¡æ¯
                if username not in integrated_data['kol_activity']:
                    integrated_data['kol_activity'][username] = {
                        'username': username,
                        'rank': kol_info.get('rank'),
                        'followers': kol_info.get('followers'),
                        'verified': kol_info.get('verified', False),
                        'score': kol_info.get('score'),
                    }

        # ä¿å­˜æŒ‰å‘¨åˆ†ç»„çš„æ¨æ–‡
        integrated_data['tweets_by_week'][week_name] = week_tweets

        # å‘¨æ‘˜è¦
        week_summary = {
            'week_name': week_name,
            'date_range': {
                'start': report.get('start_date'),
                'end': report.get('end_date'),
            },
            'tweet_count': len(week_tweets),
            'unique_kols': len(set(t.get('kol_info', {}).get('username') for t in week_tweets if t.get('kol_info'))),
        }

        integrated_data['weekly_summaries'].append(week_summary)
        integrated_data['statistics']['total_tweets'] += len(week_tweets)

        # æ›´æ–°æ—¥æœŸèŒƒå›´
        start_date = report.get('start_date')
        end_date = report.get('end_date')

        if start_date:
            if not integrated_data['statistics']['date_range']['earliest'] or \
               start_date < integrated_data['statistics']['date_range']['earliest']:
                integrated_data['statistics']['date_range']['earliest'] = start_date

        if end_date:
            if not integrated_data['statistics']['date_range']['latest'] or \
               end_date > integrated_data['statistics']['date_range']['latest']:
                integrated_data['statistics']['date_range']['latest'] = end_date

    # æ›´æ–° KOL ç»Ÿè®¡
    for username, stats in kol_stats.items():
        if username in integrated_data['kol_activity']:
            integrated_data['kol_activity'][username].update({
                'total_tweets': stats['tweet_count'],
                'total_likes': stats['total_likes'],
                'total_retweets': stats['total_retweets'],
                'weeks_active': stats['weeks'],
                'avg_likes_per_tweet': round(stats['total_likes'] / stats['tweet_count'], 1) if stats['tweet_count'] > 0 else 0,
            })

    integrated_data['statistics']['total_kols'] = len(all_kols)

    return integrated_data


def generate_integration_report(integrated_data: Dict, output_path: str):
    """ç”Ÿæˆé›†æˆæŠ¥å‘Š"""

    metadata = integrated_data['metadata']
    stats = integrated_data['statistics']

    report = f"""# Twitter åŸå§‹æ•°æ®é›†æˆæŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {metadata['integration_date']}
**æ•°æ®æ¥æº**: {metadata['total_weeks']} ä¸ªå‘¨æŠ¥

---

## ğŸ“Š æ€»ä½“ç»Ÿè®¡

- **æ¨æ–‡æ€»æ•°**: {stats['total_tweets']:,} æ¡
- **KOL æ€»æ•°**: {stats['total_kols']} ä½
- **æ•°æ®æ—¶é—´èŒƒå›´**: {stats['date_range']['earliest']} è‡³ {stats['date_range']['latest']}

---

## ğŸ“… å‘¨æŠ¥æ•°æ®è¯¦æƒ…

"""

    for week in integrated_data['weekly_summaries']:
        report += f"""### {week['week_name']}
**æ—¶é—´**: {week['date_range']['start']} è‡³ {week['date_range']['end']}
- æ¨æ–‡æ•°: {week['tweet_count']:,} æ¡
- KOL æ•°: {week['unique_kols']} ä½

"""

    report += """
---

## ğŸ‘¥ æœ€æ´»è·ƒ KOL Top 30

"""

    # æŒ‰æ¨æ–‡æ•°æ’åº
    sorted_kols = sorted(
        integrated_data['kol_activity'].items(),
        key=lambda x: x[1].get('total_tweets', 0),
        reverse=True
    )[:30]

    report += "| æ’å | KOL | æ¨æ–‡æ•° | æ´»è·ƒå‘¨æ•° | æ€»èµæ•° | æ€»è½¬å‘æ•° | å¹³å‡èµ/æ¨æ–‡ | ç²‰ä¸æ•° |\n"
    report += "|------|-----|--------|----------|--------|----------|-------------|--------|\n"

    for i, (username, data) in enumerate(sorted_kols, 1):
        report += f"| {i} | @{username} | {data.get('total_tweets', 0)} | {len(data.get('weeks_active', []))} | {data.get('total_likes', 0):,} | {data.get('total_retweets', 0):,} | {data.get('avg_likes_per_tweet', 0)} | {data.get('followers', 0):,} |\n"

    report += f"""
---

## ğŸ“‚ æ•°æ®æºè¯¦æƒ…

"""

    for source in metadata['data_sources']:
        report += f"- **{source['week']}** ({source['date_range']})\n"
        report += f"  - æ¨æ–‡: {source['tweet_count']:,} æ¡\n"
        report += f"  - æ–‡ä»¶å¤§å°: {source['file_size_mb']} MB\n\n"

    report += """
---

**ç”Ÿæˆå·¥å…·**: Claude Code - Twitter Raw Data Integration Script
**å®Œæ•´æ•°æ®**: è§ `integrated_all_tweets.json`

**æ•°æ®ç»“æ„è¯´æ˜**:
- `all_tweets`: æ‰€æœ‰æ¨æ–‡çš„å®Œæ•´åˆ—è¡¨ï¼ˆåŒ…å«æ¨æ–‡å†…å®¹ã€KOL ä¿¡æ¯ã€äº’åŠ¨æ•°æ®ï¼‰
- `tweets_by_week`: æŒ‰å‘¨åˆ†ç»„çš„æ¨æ–‡æ•°æ®
- `kol_activity`: æ¯ä½ KOL çš„è¯¦ç»†æ´»è·ƒåº¦ç»Ÿè®¡
- `weekly_summaries`: æ¯å‘¨æ•°æ®æ‘˜è¦
"""

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"âœ… é›†æˆæŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")


def main():
    # é…ç½®è·¯å¾„
    base_dir = "/Users/wenyongteng/twitter hot news/weekly_monitor/weekly_reports"
    output_dir = "/Users/wenyongteng/vibe_coding/twitter_product_trends-20251022/data_sources"

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    print("ğŸ” å¼€å§‹æ‰«ææ‰€æœ‰å‘¨æŠ¥ç›®å½•...")

    # æŸ¥æ‰¾æ‰€æœ‰ raw_data.json
    raw_data_files = find_all_raw_data_files(base_dir)

    print(f"\nğŸ“Š æ‰¾åˆ° {len(raw_data_files)} ä¸ªåŒ…å«åŸå§‹æ•°æ®çš„å‘¨æŠ¥:")
    for f in raw_data_files:
        print(f"  - {f['week_name']}: {f['file_size_mb']} MB")
    print()

    # é›†æˆæ•°æ®
    print("ğŸ”„ å¼€å§‹é›†æˆæ‰€æœ‰åŸå§‹æ¨æ–‡æ•°æ®...\n")
    integrated_data = integrate_all_raw_data(raw_data_files)

    # ä¿å­˜å®Œæ•´ JSONï¼ˆåŒ…å«æ‰€æœ‰æ¨æ–‡ï¼‰
    json_output_path = os.path.join(output_dir, "integrated_all_tweets.json")
    print(f"\nğŸ’¾ ä¿å­˜å®Œæ•´æ•°æ®...")
    with open(json_output_path, 'w', encoding='utf-8') as f:
        json.dump(integrated_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… å®Œæ•´æ•°æ®å·²ä¿å­˜: {json_output_path}")
    file_size = Path(json_output_path).stat().st_size / (1024 * 1024)
    print(f"   æ–‡ä»¶å¤§å°: {file_size:.2f} MB")

    # ç”ŸæˆæŠ¥å‘Š
    print("\nğŸ“ ç”Ÿæˆé›†æˆæŠ¥å‘Š...")
    report_output_path = os.path.join(output_dir, "all_tweets_integration_report.md")
    generate_integration_report(integrated_data, report_output_path)

    # æ‰“å°ç»Ÿè®¡æ‘˜è¦
    print("\n" + "="*70)
    print("ğŸ“Š æ•°æ®é›†æˆå®Œæˆï¼")
    print("="*70)
    print(f"æ€»æ¨æ–‡æ•°: {integrated_data['statistics']['total_tweets']:,} æ¡")
    print(f"æ€» KOL æ•°: {integrated_data['statistics']['total_kols']} ä½")
    print(f"æ•°æ®å‘¨æ•°: {integrated_data['metadata']['total_weeks']} å‘¨")
    print(f"æ—¶é—´èŒƒå›´: {integrated_data['statistics']['date_range']['earliest']} "
          f"è‡³ {integrated_data['statistics']['date_range']['latest']}")
    print(f"\nè¾“å‡ºæ–‡ä»¶:")
    print(f"  - å®Œæ•´æ¨æ–‡ JSON: {json_output_path} ({file_size:.2f} MB)")
    print(f"  - é›†æˆæŠ¥å‘Š: {report_output_path}")
    print("="*70)


if __name__ == "__main__":
    main()
