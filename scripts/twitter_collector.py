#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Twitter Data Collector - Module 1
è°ƒç”¨ twitterio API é‡‡é›† Top KOL æ¨æ–‡
"""

import json
import subprocess
import sys
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional


class TwitterCollector:
    """Twitter æ•°æ®é‡‡é›†å™¨"""

    def __init__(self, config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–é‡‡é›†å™¨

        Args:
            config: é…ç½®å­—å…¸ (å¦‚æœä¸ºNoneåˆ™ä»æ–‡ä»¶è¯»å–)
        """
        if config is None:
            config_path = Path(__file__).parent.parent / "config" / "integration_config.json"
            with open(config_path, 'r', encoding='utf-8') as f:
                full_config = json.load(f)
                config = full_config['twitter']

        self.kol_count = config.get('kol_count', 300)
        self.days = config.get('days', 7)
        self.collector_path = Path(config.get('collector_path'))
        self.output_dir = Path(config.get('output_dir'))

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def collect(self, days: Optional[int] = None, kol_count: Optional[int] = None) -> List[Dict]:
        """
        é‡‡é›† Twitter æ•°æ®

        Args:
            days: æ—¶é—´èŒƒå›´(å¤©) - è¦†ç›–é…ç½®
            kol_count: KOLæ•°é‡ - è¦†ç›–é…ç½®

        Returns:
            æ¨æ–‡åˆ—è¡¨
        """
        days = days or self.days
        kol_count = kol_count or self.kol_count

        print(f"ğŸ“± å¼€å§‹é‡‡é›† Twitter æ•°æ®...")
        print(f"   - KOL æ•°é‡: {kol_count}")
        print(f"   - æ—¶é—´èŒƒå›´: è¿‡å» {days} å¤©")

        # è®¡ç®—æ—¥æœŸèŒƒå›´
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)

        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„å†å²æ•°æ®
        existing_data = self._check_existing_data(start_date, end_date)

        if existing_data:
            print(f"   âœ… æ‰¾åˆ°å·²æœ‰æ•°æ®: {existing_data['file_path']}")
            print(f"      - æ—¥æœŸèŒƒå›´: {existing_data['start_date']} è‡³ {existing_data['end_date']}")
            print(f"      - æ¨æ–‡æ•°: {existing_data['tweet_count']}")

            # è¯¢é—®æ˜¯å¦ä½¿ç”¨å·²æœ‰æ•°æ®
            use_existing = input(f"\n   ä½¿ç”¨å·²æœ‰æ•°æ®? (y/n, é»˜è®¤y): ").strip().lower()
            if use_existing != 'n':
                return self._load_existing_data(existing_data['file_path'])

        # é‡‡é›†æ–°æ•°æ®
        raw_tweets = self._collect_new_data(days, kol_count)

        # ä¿å­˜åˆ°æœ¬åœ°
        output_file = self._save_data(raw_tweets, start_date, end_date)
        print(f"   âœ… æ•°æ®å·²ä¿å­˜: {output_file}")

        return raw_tweets

    def _check_existing_data(self, start_date: datetime, end_date: datetime) -> Optional[Dict]:
        """æ£€æŸ¥æ˜¯å¦æœ‰å·²å­˜åœ¨çš„æ•°æ®"""

        # æ£€æŸ¥æœ¬åœ° data_sources/
        for file_path in self.output_dir.glob("*_raw_tweets.json"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                metadata = data.get('metadata', {})
                date_range = metadata.get('date_range', {})

                if date_range:
                    # æ£€æŸ¥æ—¥æœŸèŒƒå›´æ˜¯å¦åŒ¹é…
                    data_start = datetime.fromisoformat(date_range['start'])
                    data_end = datetime.fromisoformat(date_range['end'])

                    # å…è®¸ Â±1 å¤©çš„è¯¯å·®
                    if abs((data_start - start_date).days) <= 1 and \
                       abs((data_end - end_date).days) <= 1:
                        return {
                            'file_path': str(file_path),
                            'start_date': date_range['start'],
                            'end_date': date_range['end'],
                            'tweet_count': metadata.get('tweet_count', len(data.get('tweets', [])))
                        }
            except Exception as e:
                continue

        # æ£€æŸ¥ twitter monitor çš„å‘¨æŠ¥ç›®å½•
        monitor_reports_dir = self.collector_path / "weekly_reports"
        if monitor_reports_dir.exists():
            for week_dir in monitor_reports_dir.glob("week_*"):
                raw_data_file = week_dir / "raw_data.json"
                if raw_data_file.exists():
                    try:
                        with open(raw_data_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)

                        metadata = data.get('metadata', {})
                        date_range = metadata.get('date_range', {})

                        if date_range:
                            data_start = datetime.fromisoformat(date_range['start'])
                            data_end = datetime.fromisoformat(date_range['end'])

                            if abs((data_start - start_date).days) <= 1 and \
                               abs((data_end - end_date).days) <= 1:
                                return {
                                    'file_path': str(raw_data_file),
                                    'start_date': date_range['start'],
                                    'end_date': date_range['end'],
                                    'tweet_count': metadata.get('total_tweets', len(data.get('tweets', [])))
                                }
                    except Exception as e:
                        continue

        return None

    def _load_existing_data(self, file_path: str) -> List[Dict]:
        """åŠ è½½å·²å­˜åœ¨çš„æ•°æ®"""
        print(f"   ğŸ“‚ åŠ è½½å·²æœ‰æ•°æ®: {file_path}")

        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        tweets = data.get('tweets', data.get('all_tweets', []))
        print(f"   âœ… åŠ è½½å®Œæˆ: {len(tweets)} æ¡æ¨æ–‡")

        return tweets

    def _collect_new_data(self, days: int, kol_count: int) -> List[Dict]:
        """é‡‡é›†æ–°æ•°æ®"""
        print(f"\n   ğŸ”„ å¼€å§‹é‡‡é›†æ–°æ•°æ®...")

        # è°ƒç”¨ twitter monitor çš„é‡‡é›†è„šæœ¬
        collect_script = self.collector_path / "collect_data.py"

        if not collect_script.exists():
            raise FileNotFoundError(f"é‡‡é›†è„šæœ¬ä¸å­˜åœ¨: {collect_script}")

        # æ‰§è¡Œé‡‡é›†å‘½ä»¤
        cmd = [
            sys.executable,
            str(collect_script),
            '--days', str(days),
            '--kol-count', str(kol_count)
        ]

        print(f"   æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        print(f"   (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...)\n")

        try:
            result = subprocess.run(
                cmd,
                cwd=str(self.collector_path),
                capture_output=True,
                text=True,
                timeout=1800  # 30åˆ†é’Ÿè¶…æ—¶
            )

            if result.returncode != 0:
                print(f"   âŒ é‡‡é›†å¤±è´¥:")
                print(result.stderr)
                raise RuntimeError(f"Twitter æ•°æ®é‡‡é›†å¤±è´¥: {result.stderr}")

            print(result.stdout)

            # æŸ¥æ‰¾æœ€æ–°ç”Ÿæˆçš„æ•°æ®æ–‡ä»¶
            latest_data_file = self._find_latest_data_file()

            if not latest_data_file:
                raise FileNotFoundError("æœªæ‰¾åˆ°é‡‡é›†çš„æ•°æ®æ–‡ä»¶")

            # åŠ è½½æ•°æ®
            with open(latest_data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            tweets = data.get('tweets', data.get('all_tweets', []))
            print(f"   âœ… é‡‡é›†å®Œæˆ: {len(tweets)} æ¡æ¨æ–‡")

            return tweets

        except subprocess.TimeoutExpired:
            raise RuntimeError("é‡‡é›†è¶…æ—¶ (30åˆ†é’Ÿ)")

    def _find_latest_data_file(self) -> Optional[Path]:
        """æŸ¥æ‰¾æœ€æ–°ç”Ÿæˆçš„æ•°æ®æ–‡ä»¶"""

        # æŸ¥æ‰¾ weekly_reports/ ä¸‹æœ€æ–°çš„ raw_data.json
        reports_dir = self.collector_path / "weekly_reports"

        if not reports_dir.exists():
            return None

        week_dirs = sorted(
            [d for d in reports_dir.glob("week_*") if d.is_dir()],
            key=lambda x: x.stat().st_mtime,
            reverse=True
        )

        for week_dir in week_dirs:
            raw_data_file = week_dir / "raw_data.json"
            if raw_data_file.exists():
                return raw_data_file

        return None

    def _save_data(self, tweets: List[Dict], start_date: datetime, end_date: datetime) -> Path:
        """ä¿å­˜æ•°æ®åˆ°æœ¬åœ°"""

        date_str = datetime.now().strftime("%Y%m%d")
        output_file = self.output_dir / f"{date_str}_raw_tweets.json"

        data = {
            "metadata": {
                "collection_date": datetime.now().isoformat(),
                "kol_count": self.kol_count,
                "tweet_count": len(tweets),
                "date_range": {
                    "start": start_date.strftime("%Y-%m-%d"),
                    "end": end_date.strftime("%Y-%m-%d")
                }
            },
            "tweets": tweets
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

        return output_file


def main():
    """æµ‹è¯•å…¥å£"""
    import argparse

    parser = argparse.ArgumentParser(description='Twitter æ•°æ®é‡‡é›†å™¨')
    parser.add_argument('--days', type=int, default=7, help='æ—¶é—´èŒƒå›´(å¤©)')
    parser.add_argument('--kol-count', type=int, default=300, help='KOLæ•°é‡')

    args = parser.parse_args()

    collector = TwitterCollector()
    tweets = collector.collect(days=args.days, kol_count=args.kol_count)

    print(f"\nâœ… é‡‡é›†å®Œæˆ!")
    print(f"   æ¨æ–‡æ•°: {len(tweets)}")


if __name__ == "__main__":
    main()
