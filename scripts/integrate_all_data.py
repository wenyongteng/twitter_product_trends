#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Twitter æ•°æ®é›†æˆå·¥å…·
å°†æ‰€æœ‰å†å²å‘¨æŠ¥æ•°æ®é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„ JSON æ–‡ä»¶ä¸­
"""

import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List


def find_all_weekly_reports(base_dir: str) -> List[Dict]:
    """æŸ¥æ‰¾æ‰€æœ‰å‘¨æŠ¥ç›®å½•"""
    reports_dir = Path(base_dir)

    if not reports_dir.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {base_dir}")
        return []

    weekly_reports = []

    for item in reports_dir.iterdir():
        if item.is_dir() and item.name.startswith('week_'):
            # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®æ–‡ä»¶
            raw_data = item / 'raw_data.json'
            summary_data = item / 'analysis_summary.json'

            report_info = {
                'directory': str(item),
                'week_name': item.name,
                'has_raw_data': raw_data.exists(),
                'has_summary': summary_data.exists(),
                'raw_data_path': str(raw_data) if raw_data.exists() else None,
                'summary_path': str(summary_data) if summary_data.exists() else None,
            }

            # æå–æ—¥æœŸèŒƒå›´
            if '_to_' in item.name:
                parts = item.name.replace('week_', '').split('_to_')
                if len(parts) == 2:
                    report_info['start_date'] = parts[0].split('_')[0]  # å»æ‰å¯èƒ½çš„æ¨¡å‹åç¼€
                    report_info['end_date'] = parts[1].split('_')[0]

            weekly_reports.append(report_info)

    # æŒ‰æ—¥æœŸæ’åº
    weekly_reports.sort(key=lambda x: x.get('start_date', ''), reverse=True)

    return weekly_reports


def load_json_safe(file_path: str) -> Dict:
    """å®‰å…¨åŠ è½½ JSON æ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"âš ï¸  åŠ è½½å¤±è´¥ {file_path}: {e}")
        return {}


def integrate_all_data(weekly_reports: List[Dict]) -> Dict:
    """é›†æˆæ‰€æœ‰æ•°æ®"""

    integrated_data = {
        'metadata': {
            'integration_date': datetime.now().isoformat(),
            'total_weeks': len(weekly_reports),
            'data_sources': [],
        },
        'weekly_reports': [],
        'aggregated_statistics': {
            'total_tweets_analyzed': 0,
            'total_products_identified': 0,
            'total_new_products': 0,
            'date_range': {
                'earliest': None,
                'latest': None,
            }
        },
        'all_products': {},  # è·¨å‘¨æœŸæ±‡æ€»çš„äº§å“æ•°æ®
        'all_kols': {},  # è·¨å‘¨æœŸæ±‡æ€»çš„ KOL æ•°æ®
    }

    all_products_mentions = {}
    all_kols_activity = {}

    for report in weekly_reports:
        if not report['has_summary']:
            continue

        print(f"ğŸ“Š å¤„ç†: {report['week_name']}")

        # åŠ è½½æ‘˜è¦æ•°æ®
        summary = load_json_safe(report['summary_path'])

        if not summary:
            continue

        # è®°å½•æ•°æ®æº
        integrated_data['metadata']['data_sources'].append({
            'week': report['week_name'],
            'date_range': f"{report.get('start_date', 'N/A')} to {report.get('end_date', 'N/A')}",
            'has_raw_data': report['has_raw_data'],
        })

        # æå–å‘¨æŠ¥æ‘˜è¦
        week_summary = summary.get('summary', {})
        products = summary.get('products', {})
        new_products = summary.get('new_products', {})

        # æ·»åŠ åˆ°å‘¨æŠ¥åˆ—è¡¨
        week_data = {
            'week_name': report['week_name'],
            'date_range': {
                'start': report.get('start_date'),
                'end': report.get('end_date'),
            },
            'statistics': {
                'total_tweets': week_summary.get('total_tweets', 0),
                'unique_products': week_summary.get('unique_products', 0),
                'new_products': week_summary.get('new_products', 0),
            },
            'top_topics': week_summary.get('top_topics', {}),
            'top_products': {},
            'new_products_list': list(new_products.keys()) if new_products else [],
        }

        # æå– Top 10 äº§å“
        sorted_products = sorted(
            products.items(),
            key=lambda x: x[1].get('mention_count', 0),
            reverse=True
        )
        week_data['top_products'] = {
            name: data.get('mention_count', 0)
            for name, data in sorted_products[:10]
        }

        integrated_data['weekly_reports'].append(week_data)

        # æ›´æ–°ç»Ÿè®¡
        integrated_data['aggregated_statistics']['total_tweets_analyzed'] += week_summary.get('total_tweets', 0)

        # æ±‡æ€»äº§å“æ•°æ®
        for product_name, product_data in products.items():
            if product_name not in all_products_mentions:
                all_products_mentions[product_name] = {
                    'total_mentions': 0,
                    'weeks_mentioned': [],
                    'total_engagement': 0,
                }

            all_products_mentions[product_name]['total_mentions'] += product_data.get('mention_count', 0)
            all_products_mentions[product_name]['weeks_mentioned'].append(report['week_name'])
            all_products_mentions[product_name]['total_engagement'] += product_data.get('total_engagement', 0)

        # æ±‡æ€» KOL æ•°æ®
        top_kols = summary.get('top_kols', {})
        for kol_name, kol_data in top_kols.items():
            if kol_name not in all_kols_activity:
                all_kols_activity[kol_name] = {
                    'total_tweets': 0,
                    'weeks_active': [],
                }

            # kol_data å¯èƒ½æ˜¯æ•´æ•°ï¼ˆæ¨æ–‡æ•°ï¼‰æˆ–å­—å…¸ï¼ˆè¯¦ç»†æ•°æ®ï¼‰
            if isinstance(kol_data, int):
                all_kols_activity[kol_name]['total_tweets'] += kol_data
            elif isinstance(kol_data, dict):
                all_kols_activity[kol_name]['total_tweets'] += kol_data.get('tweet_count', 0)

            all_kols_activity[kol_name]['weeks_active'].append(report['week_name'])

        # æ›´æ–°æ—¥æœŸèŒƒå›´
        start_date = report.get('start_date')
        end_date = report.get('end_date')

        if start_date:
            if not integrated_data['aggregated_statistics']['date_range']['earliest'] or \
               start_date < integrated_data['aggregated_statistics']['date_range']['earliest']:
                integrated_data['aggregated_statistics']['date_range']['earliest'] = start_date

        if end_date:
            if not integrated_data['aggregated_statistics']['date_range']['latest'] or \
               end_date > integrated_data['aggregated_statistics']['date_range']['latest']:
                integrated_data['aggregated_statistics']['date_range']['latest'] = end_date

    # æ’åºå¹¶æ·»åŠ æ±‡æ€»æ•°æ®
    integrated_data['all_products'] = dict(
        sorted(all_products_mentions.items(), key=lambda x: x[1]['total_mentions'], reverse=True)
    )

    integrated_data['all_kols'] = dict(
        sorted(all_kols_activity.items(), key=lambda x: x[1]['total_tweets'], reverse=True)
    )

    # æ›´æ–°ç»Ÿè®¡
    integrated_data['aggregated_statistics']['total_products_identified'] = len(all_products_mentions)

    return integrated_data


def generate_integration_report(integrated_data: Dict, output_path: str):
    """ç”Ÿæˆé›†æˆæŠ¥å‘Šï¼ˆMarkdown æ ¼å¼ï¼‰"""

    metadata = integrated_data['metadata']
    stats = integrated_data['aggregated_statistics']

    report = f"""# Twitter æ•°æ®é›†æˆæŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {metadata['integration_date']}
**æ•°æ®æ¥æº**: {metadata['total_weeks']} ä¸ªå‘¨æŠ¥

---

## ğŸ“Š æ€»ä½“ç»Ÿè®¡

- **åˆ†ææ¨æ–‡æ€»æ•°**: {stats['total_tweets_analyzed']:,} æ¡
- **è¯†åˆ«äº§å“æ€»æ•°**: {stats['total_products_identified']} ä¸ª
- **æ•°æ®æ—¶é—´èŒƒå›´**: {stats['date_range']['earliest']} è‡³ {stats['date_range']['latest']}

---

## ğŸ“… å‘¨æŠ¥åˆ—è¡¨

"""

    for week in integrated_data['weekly_reports']:
        report += f"""### {week['week_name']}
**æ—¶é—´**: {week['date_range']['start']} è‡³ {week['date_range']['end']}
- æ¨æ–‡æ•°: {week['statistics']['total_tweets']:,}
- äº§å“æ•°: {week['statistics']['unique_products']}
- æ–°äº§å“: {week['statistics']['new_products']}

**Top 5 äº§å“**:
"""
        top_5 = list(week['top_products'].items())[:5]
        for i, (product, count) in enumerate(top_5, 1):
            report += f"{i}. {product}: {count}æ¬¡\n"

        report += "\n"

    report += f"""
---

## ğŸ† è·¨å‘¨æœŸçƒ­é—¨äº§å“ Top 20

"""

    top_products = list(integrated_data['all_products'].items())[:20]
    report += "| æ’å | äº§å“åç§° | æ€»æåŠæ¬¡æ•° | å‡ºç°å‘¨æ•° | æ€»äº’åŠ¨æ•° |\n"
    report += "|------|----------|------------|----------|----------|\n"

    for i, (product_name, data) in enumerate(top_products, 1):
        report += f"| {i} | {product_name} | {data['total_mentions']} | {len(data['weeks_mentioned'])} | {data['total_engagement']:,} |\n"

    report += f"""
---

## ğŸ‘¥ è·¨å‘¨æœŸæ´»è·ƒ KOL Top 20

"""

    top_kols = list(integrated_data['all_kols'].items())[:20]
    report += "| æ’å | KOL | æ€»æ¨æ–‡æ•° | æ´»è·ƒå‘¨æ•° |\n"
    report += "|------|-----|----------|----------|\n"

    for i, (kol_name, data) in enumerate(top_kols, 1):
        report += f"| {i} | @{kol_name} | {data['total_tweets']} | {len(data['weeks_active'])} |\n"

    report += f"""
---

## ğŸ“‚ æ•°æ®æº

"""

    for source in metadata['data_sources']:
        has_raw = "âœ…" if source['has_raw_data'] else "âŒ"
        report += f"- {source['week']} ({source['date_range']}) - åŸå§‹æ•°æ®: {has_raw}\n"

    report += """
---

**ç”Ÿæˆå·¥å…·**: Claude Code - Twitter Data Integration Script
**å®Œæ•´æ•°æ®**: è§ `integrated_twitter_data.json`
"""

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"âœ… é›†æˆæŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")


def main():
    # é…ç½®è·¯å¾„
    base_dir = "/Users/wenyongteng/twitter hot news/weekly_monitor/weekly_reports"
    output_dir = "/Users/wenyongteng/vibe_coding/twitter_product_trends-20251022/data_sources"

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    print("ğŸ” å¼€å§‹æ‰«æå‘¨æŠ¥æ•°æ®...")

    # æŸ¥æ‰¾æ‰€æœ‰å‘¨æŠ¥
    weekly_reports = find_all_weekly_reports(base_dir)

    print(f"ğŸ“Š æ‰¾åˆ° {len(weekly_reports)} ä¸ªå‘¨æŠ¥ç›®å½•")
    print(f"   - æœ‰æ‘˜è¦æ•°æ®: {sum(1 for r in weekly_reports if r['has_summary'])} ä¸ª")
    print(f"   - æœ‰åŸå§‹æ•°æ®: {sum(1 for r in weekly_reports if r['has_raw_data'])} ä¸ª")
    print()

    # é›†æˆæ•°æ®
    print("ğŸ”„ å¼€å§‹é›†æˆæ•°æ®...")
    integrated_data = integrate_all_data(weekly_reports)

    # ä¿å­˜ JSON
    json_output_path = os.path.join(output_dir, "integrated_twitter_data.json")
    with open(json_output_path, 'w', encoding='utf-8') as f:
        json.dump(integrated_data, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… é›†æˆæ•°æ®å·²ä¿å­˜: {json_output_path}")

    # ç”ŸæˆæŠ¥å‘Š
    print("\nğŸ“ ç”Ÿæˆé›†æˆæŠ¥å‘Š...")
    report_output_path = os.path.join(output_dir, "integration_report.md")
    generate_integration_report(integrated_data, report_output_path)

    # æ‰“å°ç»Ÿè®¡æ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ“Š æ•°æ®é›†æˆå®Œæˆï¼")
    print("="*60)
    print(f"æ€»æ¨æ–‡æ•°: {integrated_data['aggregated_statistics']['total_tweets_analyzed']:,}")
    print(f"æ€»äº§å“æ•°: {integrated_data['aggregated_statistics']['total_products_identified']}")
    print(f"æ—¶é—´èŒƒå›´: {integrated_data['aggregated_statistics']['date_range']['earliest']} "
          f"è‡³ {integrated_data['aggregated_statistics']['date_range']['latest']}")
    print(f"\nè¾“å‡ºæ–‡ä»¶:")
    print(f"  - JSON æ•°æ®: {json_output_path}")
    print(f"  - Markdown æŠ¥å‘Š: {report_output_path}")
    print("="*60)


if __name__ == "__main__":
    main()
