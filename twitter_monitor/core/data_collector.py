"""
æ•°æ®é‡‡é›†æ¨¡å—
è´Ÿè´£æ”¶é›†Top N KOLè¿‡å»Nå¤©çš„æ¨æ–‡æ•°æ®
"""

import sys
import os
from datetime import datetime, timedelta
import json

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from twitter_collector import TwitterCollector
from config.config import DATA_COLLECTION


class KOLWeeklyDataCollector:
    """
    KOLå‘¨åº¦æ•°æ®é‡‡é›†å™¨
    """

    def __init__(self, api_key=None):
        """
        åˆå§‹åŒ–é‡‡é›†å™¨

        Args:
            api_key: Twitter APIå¯†é’¥
        """
        # ä½¿ç”¨Twitter APIå¯†é’¥ï¼ˆä¸æ˜¯Claude APIå¯†é’¥ï¼‰
        self.api_key = api_key or 'e734db59d601492e9406f6b6d30c22aa'
        self.collector = TwitterCollector(self.api_key)

        # åŠ è½½KOLæ•°æ®
        self.kol_data = self._load_kol_data()

    def _load_kol_data(self):
        """åŠ è½½KOLæ•°æ®"""
        import csv

        # ä¼˜å…ˆä½¿ç”¨weekly_monitorç›®å½•ä¸‹çš„product kol_ranking_weighted.csv
        kol_file = os.path.join(
            os.path.dirname(os.path.dirname(__file__)),
            'product kol_ranking_weighted.csv'
        )

        # å¦‚æœä¸å­˜åœ¨ï¼Œå°è¯•æ—§è·¯å¾„
        if not os.path.exists(kol_file):
            kol_file = os.path.join(
                os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
                'kol_analysis/kol_ranking_weighted.csv'
            )

        kols = []
        with open(kol_file, 'r', encoding='utf-8-sig') as f:  # utf-8-sig è‡ªåŠ¨å»é™¤BOM
            reader = csv.DictReader(f)
            for row in reader:
                kols.append({
                    'username': row['Twitterç”¨æˆ·å'],
                    'rank': int(row['åŠ æƒæ’å']),
                    'score': float(row['åŠ æƒæ€»åˆ†']),
                    'followers': int(row['ç²‰ä¸æ•°']) if row.get('ç²‰ä¸æ•°') else 0,
                    'verified': row.get('å®˜æ–¹è®¤è¯', '') == 'âœ“' or row.get('è“Vè®¤è¯', '') == 'âœ“',
                })

        return kols

    def _filter_by_date(self, tweets, start_date, end_date):
        """
        æŒ‰æ—¥æœŸè¿‡æ»¤æ¨æ–‡

        Args:
            tweets: æ¨æ–‡åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            list: è¿‡æ»¤åçš„æ¨æ–‡
        """
        from dateutil import parser

        filtered = []
        for tweet in tweets:
            created_at = tweet.get('created_at') or tweet.get('createdAt')
            if not created_at:
                continue

            try:
                # è§£ææ¨æ–‡æ—¶é—´
                tweet_date = parser.parse(created_at)
                # ç§»é™¤æ—¶åŒºä¿¡æ¯è¿›è¡Œæ¯”è¾ƒ
                tweet_date = tweet_date.replace(tzinfo=None)

                # æ£€æŸ¥æ˜¯å¦åœ¨èŒƒå›´å†…
                if start_date <= tweet_date <= end_date:
                    filtered.append(tweet)
            except Exception as e:
                # è§£æå¤±è´¥ï¼Œè·³è¿‡è¯¥æ¨æ–‡
                continue

        return filtered

    def get_top_kols(self, n=300):
        """
        è·å–Top N KOL

        Args:
            n: æ•°é‡

        Returns:
            list: Top N KOLåˆ—è¡¨
        """
        return sorted(self.kol_data, key=lambda x: x['rank'])[:n]

    def collect_weekly_tweets(self, days=7, kol_count=300):
        """
        æ”¶é›†KOLå‘¨åº¦æ¨æ–‡

        Args:
            days: è¿‡å»Nå¤©
            kol_count: KOLæ•°é‡

        Returns:
            dict: {
                'tweets': [...],
                'metadata': {...}
            }
        """
        print(f"ğŸ“Š å¼€å§‹æ”¶é›†æ•°æ®...")
        print(f"   - KOLèŒƒå›´: Top {kol_count}")
        print(f"   - æ—¶é—´èŒƒå›´: è¿‡å»{days}å¤©")

        # è®¡ç®—æ—¶é—´èŒƒå›´
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)

        # è·å–Top KOL
        top_kols = self.get_top_kols(kol_count)
        print(f"   - å·²åŠ è½½ {len(top_kols)} ä¸ªKOL")

        # APIæˆæœ¬è·Ÿè¸ª
        api_calls = 0  # APIè°ƒç”¨æ¬¡æ•°

        # æ”¶é›†æ¨æ–‡
        all_tweets = []
        kol_tweet_count = {}

        for i, kol in enumerate(top_kols, 1):
            username = kol['username']

            try:
                # ä½¿ç”¨æ–°çš„ç”¨æˆ·æ¨æ–‡æ”¶é›†æ–¹æ³•
                tweets, calls = self.collector.collect_user_tweets(
                    username=username,
                    max_tweets=50,  # æ¯ä¸ªKOLæœ€å¤š50æ¡
                    include_replies=False  # ä¸åŒ…å«å›å¤
                )
                api_calls += calls  # ç´¯åŠ APIè°ƒç”¨æ¬¡æ•°

                # è¿‡æ»¤æ—¶é—´èŒƒå›´
                tweets = self._filter_by_date(tweets, start_date, end_date)

                # è¿‡æ»¤è½¬å‘ï¼ˆå¦‚æœé…ç½®è¦æ±‚ï¼‰
                if DATA_COLLECTION['exclude_retweets']:
                    tweets = [t for t in tweets if not t.get('text', '').startswith('RT @')]

                # è¿‡æ»¤ä½äº’åŠ¨ï¼ˆå¦‚æœé…ç½®è¦æ±‚ï¼‰
                min_engagement = DATA_COLLECTION.get('min_engagement', 0)
                if min_engagement > 0:
                    tweets = [
                        t for t in tweets
                        if (t.get('public_metrics', {}).get('like_count', 0) +
                            t.get('public_metrics', {}).get('retweet_count', 0)) >= min_engagement
                    ]

                # æ·»åŠ KOLä¿¡æ¯åˆ°æ¯æ¡æ¨æ–‡
                for tweet in tweets:
                    tweet['kol_info'] = {
                        'username': username,
                        'rank': kol['rank'],
                        'score': kol['score'],
                        'is_top_100': kol['rank'] <= 100,
                        'followers': kol['followers'],
                        'verified': kol['verified'],
                    }

                all_tweets.extend(tweets)
                kol_tweet_count[username] = len(tweets)

                if i % 10 == 0:
                    print(f"   è¿›åº¦: {i}/{len(top_kols)} KOL, å·²æ”¶é›† {len(all_tweets)} æ¡æ¨æ–‡")

            except Exception as e:
                print(f"   âš ï¸ æ”¶é›† {username} çš„æ¨æ–‡å¤±è´¥: {e}")
                continue

        # è®¡ç®—APIæˆæœ¬
        # æ¯æ¬¡è°ƒç”¨ /twitter/user/last_tweets æ¶ˆè€— 300 credits
        # $20 å¯ä»¥ä¹° 2,000,000 credits
        credits_per_call = 300
        credits_per_dollar = 2000000 / 20  # 100,000 credits per dollar

        total_credits = api_calls * credits_per_call
        total_cost_usd = total_credits / credits_per_dollar

        # æ„å»ºå…ƒæ•°æ®
        metadata = {
            'total_tweets': len(all_tweets),
            'kol_count': len(top_kols),
            'kol_with_tweets': len([k for k in kol_tweet_count.values() if k > 0]),
            'date_range': {
                'start': start_date.strftime('%Y-%m-%d'),
                'end': end_date.strftime('%Y-%m-%d'),
            },
            'collection_time': datetime.now().isoformat(),
            'kol_tweet_distribution': kol_tweet_count,
            'api_usage': {
                'api_calls': api_calls,
                'total_credits': total_credits,
                'cost_usd': round(total_cost_usd, 4),
            }
        }

        print(f"\nâœ… æ•°æ®æ”¶é›†å®Œæˆ!")
        print(f"   - æ€»æ¨æ–‡æ•°: {metadata['total_tweets']}")
        print(f"   - æœ‰æ¨æ–‡çš„KOL: {metadata['kol_with_tweets']}/{metadata['kol_count']}")
        print(f"\nğŸ’° APIæˆæœ¬:")
        print(f"   - APIè°ƒç”¨æ¬¡æ•°: {api_calls}")
        print(f"   - æ¶ˆè€—Credits: {total_credits:,}")
        print(f"   - æˆæœ¬: ${total_cost_usd:.4f} USD")

        return {
            'tweets': all_tweets,
            'metadata': metadata,
        }

    def build_indexes(self, data):
        """
        æ„å»ºæ•°æ®ç´¢å¼•ï¼ˆåŠ é€Ÿåç»­æŸ¥è¯¢ï¼‰

        Args:
            data: é‡‡é›†çš„æ•°æ®

        Returns:
            dict: ç´¢å¼•æ•°æ®
        """
        tweets = data['tweets']

        # KOLç´¢å¼•
        kol_index = {}
        for tweet in tweets:
            username = tweet['kol_info']['username']
            if username not in kol_index:
                kol_index[username] = []
            kol_index[username].append(tweet)

        # æ—¥æœŸç´¢å¼•
        date_index = {}
        for tweet in tweets:
            date = tweet.get('created_at', '')[:10]  # YYYY-MM-DD
            if date not in date_index:
                date_index[date] = []
            date_index[date].append(tweet)

        # Top100 KOLçš„æ¨æ–‡
        top100_tweets = [t for t in tweets if t['kol_info']['is_top_100']]

        return {
            'kol_index': kol_index,
            'date_index': date_index,
            'top100_tweets': top100_tweets,
            'all_tweets': tweets,
        }

    def save_raw_data(self, data, output_dir):
        """
        ä¿å­˜åŸå§‹æ•°æ®

        Args:
            data: æ•°æ®
            output_dir: è¾“å‡ºç›®å½•
        """
        os.makedirs(output_dir, exist_ok=True)

        # ä¿å­˜ä¸ºJSON
        output_file = os.path.join(output_dir, 'raw_data.json')
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ åŸå§‹æ•°æ®å·²ä¿å­˜: {output_file}")

        return output_file


if __name__ == '__main__':
    # æµ‹è¯•
    collector = KOLWeeklyDataCollector()

    # æ”¶é›†æ•°æ®
    data = collector.collect_weekly_tweets(days=7, kol_count=10)  # æµ‹è¯•ç”¨10ä¸ªKOL

    # æ„å»ºç´¢å¼•
    indexes = collector.build_indexes(data)

    print(f"\nç´¢å¼•ç»Ÿè®¡:")
    print(f"  - KOLæ•°é‡: {len(indexes['kol_index'])}")
    print(f"  - æ—¥æœŸæ•°é‡: {len(indexes['date_index'])}")
    print(f"  - Top100æ¨æ–‡: {len(indexes['top100_tweets'])}")
