"""
äº§å“æå–æ¨¡å—
ä»åŒ…å«ä¿¡å·çš„æ¨æ–‡ä¸­æå–äº§å“åç§°
"""

import re
from config.config import EXCLUDE_TERMS


class ProductExtractor:
    """
    äº§å“åç§°æå–å™¨ï¼ˆåŸºäºä¿¡å·è¯ï¼‰
    """

    def __init__(self):
        self.exclude_terms = EXCLUDE_TERMS

    def extract_products_from_signaled_tweet(self, tweet, signals):
        """
        ä»åŒ…å«ä¿¡å·çš„æ¨æ–‡ä¸­æå–äº§å“å

        Args:
            tweet: æ¨æ–‡æ•°æ®
            signals: æ£€æµ‹åˆ°çš„ä¿¡å·åˆ—è¡¨

        Returns:
            list: æå–çš„äº§å“å€™é€‰
        """
        text = tweet.get('text', '')
        products = []

        for signal_info in signals:
            signal = signal_info['signal']
            category = signal_info['category']

            # æ ¹æ®ä¿¡å·ç±»åˆ«é€‰æ‹©æå–ç­–ç•¥
            if category in ['launch', 'announcement']:
                candidates = self._extract_near_signal(text, signal)

            elif category == 'new':
                candidates = self._extract_after_new(text, signal)

            elif category == 'comparison':
                candidates = self._extract_comparison_products(text, signal)

            elif category in ['testing', 'availability']:
                candidates = self._extract_action_target(text, signal)

            else:
                # é€šç”¨æå–ï¼šä¿¡å·è¯é™„è¿‘çš„å¤§å†™è¯ç»„
                candidates = self._extract_capitalized_near(text, signal)

            # è®°å½•å€™é€‰äº§å“
            for name in candidates:
                # è¿‡æ»¤æ’é™¤è¯
                if name in self.exclude_terms:
                    continue

                # è¿‡æ»¤å¤ªçŸ­çš„è¯
                if len(name) < 2:
                    continue

                products.append({
                    'product_name': name.strip(),
                    'tweet_id': tweet.get('id', ''),
                    'tweet_text': text,
                    'author': tweet.get('author_id', ''),
                    'signal_category': category,
                    'signal_word': signal,
                    'confidence': self._calculate_initial_confidence(category, name),
                })

        return products

    def _extract_near_signal(self, text, signal):
        """
        æå–ä¿¡å·è¯é™„è¿‘çš„äº§å“å

        ä¾‹å¦‚:
        - "Google just released Gemini 2.0" â†’ ["Gemini 2.0"]
        - "Anthropic announced Claude 3.5" â†’ ["Claude 3.5"]
        """
        pattern = re.escape(signal)
        match = re.search(pattern, text, re.IGNORECASE)

        if not match:
            return []

        signal_pos = match.start()
        products = []

        # ç­–ç•¥1: ä¿¡å·è¯ä¹‹åçš„è¯ç»„
        after_text = text[signal_pos + len(signal):].strip()
        # åŒ¹é…å¤§å†™å¼€å¤´çš„è¯ç»„ï¼Œç›´åˆ°é‡åˆ°ç‰¹å®šè¯æˆ–æ ‡ç‚¹
        after_match = re.match(
            r'^[,\s]*([A-Z][A-Za-z0-9\s\.\-]+?)(?:\s+(?:is|has|can|will|for|with|by|and|or|,|\.|!|\?|:|;))',
            after_text
        )

        # ç­–ç•¥2: ä¿¡å·è¯ä¹‹å‰çš„è¯ç»„
        before_text = text[:signal_pos].strip()
        before_match = re.search(r'([A-Z][A-Za-z0-9\s\.\-]+?)\s*$', before_text)

        if after_match:
            products.append(after_match.group(1).strip())
        if before_match:
            products.append(before_match.group(1).strip())

        return products

    def _extract_after_new(self, text, signal='new'):
        """
        æå– "new X" ä¸­çš„X

        ä¾‹å¦‚:
        - "the new Claude model" â†’ ["Claude"]
        - "introducing new Gemini" â†’ ["Gemini"]
        """
        pattern = rf'\b{re.escape(signal)}\s+([A-Z][A-Za-z0-9\s\-\.]+?)(?:\s+(?:is|has|can|model|tool|AI|from|by|,|\.|!|\?))'
        matches = re.finditer(pattern, text, re.IGNORECASE)

        products = []
        for match in matches:
            product_phrase = match.group(1).strip()

            # æå–é¦–ä¸ªå¤§å†™è¯ï¼ˆå¯èƒ½å°±æ˜¯äº§å“åï¼‰
            first_word_match = re.match(r'^([A-Z][A-Za-z0-9\-\.]+)', product_phrase)
            if first_word_match:
                products.append(first_word_match.group(1))

            # å¦‚æœæ˜¯å¤šä¸ªè¯ï¼Œä¹Ÿä¿ç•™å®Œæ•´çŸ­è¯­
            if len(product_phrase.split()) > 1:
                products.append(product_phrase)

        return products

    def _extract_comparison_products(self, text, signal):
        """
        ä»å¯¹æ¯”å¥ä¸­æå–äº§å“

        ä¾‹å¦‚:
        - "X vs Y" â†’ [X, Y]
        - "better than X" â†’ [X]
        """
        products = []

        if signal.lower() == 'vs':
            # "X vs Y" æ¨¡å¼
            pattern = r'([A-Z][A-Za-z0-9\s\-\.]+?)\s+vs\.?\s+([A-Z][A-Za-z0-9\s\-\.]+?)(?:\s|,|\.|\?|!|$)'
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                products.extend([match.group(1).strip(), match.group(2).strip()])

        elif 'better than' in signal.lower() or 'beats' in signal.lower():
            # "better than X" æ¨¡å¼
            pattern = rf'{re.escape(signal)}\s+([A-Z][A-Za-z0-9\s\-\.]+?)(?:\s|,|\.|\?|!|$)'
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                products.append(match.group(1).strip())

        elif 'alternative to' in signal.lower() or 'competitor to' in signal.lower():
            # "alternative to X" æ¨¡å¼
            pattern = rf'{re.escape(signal)}\s+([A-Z][A-Za-z0-9\s\-\.]+?)(?:\s|,|\.|\?|!|$)'
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                products.append(match.group(1).strip())

        return products

    def _extract_action_target(self, text, signal):
        """
        æå–åŠ¨ä½œçš„ç›®æ ‡äº§å“

        ä¾‹å¦‚:
        - "tried X" â†’ [X]
        - "using X" â†’ [X]
        """
        pattern = rf'{re.escape(signal)}\s+([A-Z][A-Za-z0-9\s\-\.]+?)(?:\s+(?:and|or|,|\.|!|\?|:|;)|$)'
        match = re.search(pattern, text, re.IGNORECASE)

        if match:
            return [match.group(1).strip()]

        return []

    def _extract_capitalized_near(self, text, signal, window=100):
        """
        æå–ä¿¡å·è¯é™„è¿‘çš„å¤§å†™è¯ç»„ï¼ˆé€šç”¨æ–¹æ³•ï¼‰

        Args:
            text: æ–‡æœ¬
            signal: ä¿¡å·è¯
            window: çª—å£å¤§å°

        Returns:
            list: äº§å“å€™é€‰
        """
        match = re.search(re.escape(signal), text, re.IGNORECASE)
        if not match:
            return []

        pos = match.start()
        start = max(0, pos - window)
        end = min(len(text), pos + len(signal) + window)
        context = text[start:end]

        # æå–æ‰€æœ‰å¤§å†™å¼€å¤´çš„è¯ç»„
        pattern = r'\b([A-Z][A-Za-z0-9\-\.]+(?:\s+[A-Z][A-Za-z0-9\-\.]+)*)\b'
        matches = re.finditer(pattern, context)

        products = []
        for match in matches:
            product = match.group(1).strip()
            if product and len(product) > 1:
                products.append(product)

        return products

    def _calculate_initial_confidence(self, category, product_name):
        """
        è®¡ç®—åˆå§‹ç½®ä¿¡åº¦

        Args:
            category: ä¿¡å·ç±»åˆ«
            product_name: äº§å“å

        Returns:
            float: ç½®ä¿¡åº¦ (0-1)
        """
        # ä¸åŒä¿¡å·ç±»åˆ«çš„åŸºç¡€ç½®ä¿¡åº¦
        category_confidence = {
            'launch': 0.9,      # å‘å¸ƒä¿¡å·æœ€å¯é 
            'announcement': 0.9,
            'new': 0.7,
            'comparison': 0.6,  # å¯¹æ¯”å¯èƒ½æåˆ°å·²çŸ¥äº§å“
            'testing': 0.6,
            'availability': 0.7,
            'excitement': 0.3,  # æƒ…ç»ªè¯ä¸å¤ªå¯é 
        }

        base_confidence = category_confidence.get(category, 0.5)

        # æ ¹æ®äº§å“åç‰¹å¾è°ƒæ•´
        # åŒ…å«ç‰ˆæœ¬å·ï¼ˆæ›´å¯é ï¼‰
        if re.search(r'\d+\.?\d*', product_name):
            base_confidence += 0.1

        # åŒ…å«å¸¸è§åç¼€ï¼ˆæ›´å¯é ï¼‰
        if any(suffix in product_name.lower() for suffix in [' ai', '-ai', ' model', ' tool']):
            base_confidence += 0.1

        return min(base_confidence, 1.0)

    def deduplicate_candidates(self, candidates):
        """
        å»é‡å€™é€‰äº§å“

        Args:
            candidates: å€™é€‰åˆ—è¡¨

        Returns:
            list: å»é‡åçš„å€™é€‰
        """
        seen = set()
        unique_candidates = []

        for candidate in candidates:
            # æ ‡å‡†åŒ–äº§å“åï¼ˆå¿½ç•¥å¤§å°å†™å’Œç©ºæ ¼ï¼‰
            normalized = candidate['product_name'].lower().replace(' ', '')

            if normalized not in seen:
                seen.add(normalized)
                unique_candidates.append(candidate)

        return unique_candidates


if __name__ == '__main__':
    # æµ‹è¯•
    extractor = ProductExtractor()

    test_cases = [
        {
            'text': 'OpenAI just released GPT-5, and it\'s amazing!',
            'signals': [{'category': 'launch', 'signal': 'just released', 'position': 7}]
        },
        {
            'text': 'Google announced new Gemini 2.0 model today',
            'signals': [
                {'category': 'announcement', 'signal': 'announced', 'position': 7},
                {'category': 'new', 'signal': 'new', 'position': 17}
            ]
        },
        {
            'text': 'Claude Code vs Cursor - which one is better?',
            'signals': [{'category': 'comparison', 'signal': 'vs', 'position': 12}]
        },
    ]

    print("ğŸ”¬ äº§å“æå–æµ‹è¯•:\n")

    for i, case in enumerate(test_cases, 1):
        tweet = {'text': case['text'], 'id': f'test_{i}'}
        signals = case['signals']

        print(f"æµ‹è¯• {i}:")
        print(f"  æ¨æ–‡: {case['text']}")
        print(f"  ä¿¡å·: {[s['signal'] for s in signals]}")

        products = extractor.extract_products_from_signaled_tweet(tweet, signals)

        print(f"  æå–ç»“æœ:")
        for p in products:
            print(f"    - {p['product_name']} (ä¿¡å·: {p['signal_word']}, ç½®ä¿¡åº¦: {p['confidence']})")
        print()
