#!/usr/bin/env python3
"""
æ¨æ–‡åˆ†æè„šæœ¬
æå–äº§å“ã€è¶‹åŠ¿å’Œå…³é”®ä¿¡æ¯
"""

import json
import re
from collections import defaultdict, Counter
from typing import List, Dict, Set
from datetime import datetime

def load_data(file_path: str) -> Dict:
    """åŠ è½½æ¨æ–‡æ•°æ®"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def extract_products(text: str) -> List[str]:
    """æå–äº§å“/å·¥å…·åç§°"""
    # å¸¸è§äº§å“å…³é”®è¯æ¨¡å¼
    product_patterns = [
        # AIå·¥å…·
        r'\b(ChatGPT|GPT-?[0-9o]+|Claude|Gemini|Copilot|Cursor|Codeium|V0|Bolt|Windsurf|Lovable|Replit|Midjourney|DALL-E|Stable Diffusion|RunwayML|Suno|Udio)\b',
        # å¼€å‘å·¥å…·
        r'\b(VS Code|WebStorm|IntelliJ|Xcode|Figma|Framer|Webflow|Notion|Linear|Slack|Discord|Telegram)\b',
        # AIæ¨¡å‹
        r'\b(Llama ?[0-9.]+|DeepSeek|Qwen|GLM-?[0-9.]+|Mistral|Falcon|Mixtral|Grok)\b',
        # AIå¹³å°
        r'\b(HuggingFace|Replicate|Together AI|Anthropic|OpenAI|Google AI|Perplexity|Poe|Character\.AI)\b',
        # æ–°äº§å“å…³é”®è¯
        r'(launched|releasing|introduced|unveils?|announces?|debuts?|drops?)\s+([A-Z][a-zA-Z]+(?:\s+[A-Z][a-zA-Z]+)?)',
        # @mentions of products
        r'@([a-zA-Z0-9_]+)',
    ]

    products = set()
    text_lower = text.lower()

    for pattern in product_patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            if len(match.groups()) > 0:
                products.add(match.group(1) if match.group(1) else match.group(0))
            else:
                products.add(match.group(0))

    return list(products)

def is_new_product_mention(text: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦æåˆ°æ–°äº§å“"""
    new_product_keywords = [
        'launch', 'just released', 'announcing', 'new', 'released',
        'unveil', 'debut', 'introduce', 'coming soon', 'now available',
        'åˆšå‘å¸ƒ', 'æ–°æ¨å‡º', 'æ–°åŠŸèƒ½', 'å³å°†å‘å¸ƒ', 'ä»Šå¤©å‘å¸ƒ'
    ]

    text_lower = text.lower()
    return any(keyword in text_lower for keyword in new_product_keywords)

def get_sentiment(text: str) -> str:
    """ç®€å•æƒ…æ„Ÿåˆ†æ"""
    positive_words = ['love', 'amazing', 'great', 'awesome', 'excellent', 'fantastic', 'incredible', 'best']
    negative_words = ['hate', 'terrible', 'awful', 'bad', 'poor', 'disappointed', 'worst', 'sucks']

    text_lower = text.lower()
    pos_count = sum(1 for word in positive_words if word in text_lower)
    neg_count = sum(1 for word in negative_words if word in text_lower)

    if pos_count > neg_count:
        return 'positive'
    elif neg_count > pos_count:
        return 'negative'
    else:
        return 'neutral'

def analyze_tweets(data_file: str) -> Dict:
    """åˆ†ææ¨æ–‡"""
    print("ğŸ“Š å¼€å§‹åˆ†ææ¨æ–‡...")

    data = load_data(data_file)
    tweets = data['tweets']

    print(f"æ€»æ¨æ–‡æ•°: {len(tweets)}")

    # äº§å“ç»Ÿè®¡
    product_mentions = defaultdict(list)  # product -> [tweets]
    new_products = defaultdict(list)

    # è¯é¢˜ç»Ÿè®¡
    topics = Counter()

    # KOLæ´»è·ƒåº¦
    top_kol_tweets = defaultdict(int)

    # æŒ‰æ—¥æœŸç»Ÿè®¡
    daily_tweets = defaultdict(int)

    print("å¤„ç†æ¨æ–‡ä¸­...")
    for i, tweet in enumerate(tweets, 1):
        if i % 200 == 0:
            print(f"  è¿›åº¦: {i}/{len(tweets)}")

        text = tweet.get('text', '')
        kol = tweet.get('kol_info', {})
        created_at = tweet.get('created_at', '')

        # æå–äº§å“
        products = extract_products(text)
        for product in products:
            product_mentions[product].append({
                'text': text,
                'kol': kol.get('username'),
                'rank': kol.get('rank'),
                'followers': kol.get('followers'),
                'likes': tweet.get('likeCount', 0),
                'retweets': tweet.get('retweetCount', 0),
                'created_at': created_at,
                'sentiment': get_sentiment(text),
                'is_new': is_new_product_mention(text)
            })

            if is_new_product_mention(text):
                new_products[product].append({
                    'text': text,
                    'kol': kol.get('username'),
                    'rank': kol.get('rank'),
                    'created_at': created_at
                })

        # æå–è¯é¢˜ï¼ˆç®€å•çš„å…³é”®è¯ç»Ÿè®¡ï¼‰
        keywords = ['AI', 'AGI', 'LLM', 'ML', 'agent', 'model', 'open source',
                   'coding', 'development', 'design', 'startup', 'funding']
        for keyword in keywords:
            if keyword.lower() in text.lower():
                topics[keyword] += 1

        # KOLæ´»è·ƒåº¦ï¼ˆä»…Top 100ï¼‰
        if kol.get('is_top_100'):
            top_kol_tweets[kol.get('username')] += 1

        # æ—¥æœŸç»Ÿè®¡
        try:
            date = created_at.split()[2:4]  # Mon Oct 14
            date_str = ' '.join(date)
            daily_tweets[date_str] += 1
        except:
            pass

    print("\nç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")

    # æŒ‰æåŠæ¬¡æ•°æ’åºäº§å“
    sorted_products = sorted(
        product_mentions.items(),
        key=lambda x: len(x[1]),
        reverse=True
    )

    # æŒ‰æåŠæ¬¡æ•°æ’åºæ–°äº§å“
    sorted_new_products = sorted(
        new_products.items(),
        key=lambda x: len(x[1]),
        reverse=True
    )

    # æŒ‰æ¨æ–‡æ•°æ’åºTop KOL
    sorted_top_kols = sorted(
        top_kol_tweets.items(),
        key=lambda x: x[1],
        reverse=True
    )[:20]  # Top 20

    result = {
        'summary': {
            'total_tweets': len(tweets),
            'unique_products': len(product_mentions),
            'new_products': len(new_products),
            'top_topics': dict(topics.most_common(10)),
            'date_range': data['metadata']['date_range'],
        },
        'products': {
            product: {
                'mention_count': len(mentions),
                'top_kols': sorted([m['kol'] for m in mentions if m['rank'] and m['rank'] <= 20])[:5],
                'sentiment': Counter([m['sentiment'] for m in mentions]),
                'total_engagement': sum(m['likes'] + m['retweets'] for m in mentions),
                'sample_tweets': mentions[:3]  # å‰3æ¡
            }
            for product, mentions in sorted_products[:30]  # Top 30äº§å“
        },
        'new_products': {
            product: {
                'mention_count': len(mentions),
                'first_mentioned': min([m['created_at'] for m in mentions]),
                'discoverers': [{'kol': m['kol'], 'rank': m['rank']} for m in mentions],
                'sample_tweets': [m['text'] for m in mentions[:2]]
            }
            for product, mentions in sorted_new_products[:20]  # Top 20æ–°äº§å“
        },
        'top_kols': dict(sorted_top_kols),
        'daily_distribution': dict(daily_tweets)
    }

    return result

if __name__ == '__main__':
    import sys

    data_file = sys.argv[1] if len(sys.argv) > 1 else 'weekly_reports/week_2025-10-10_to_2025-10-17/raw_data.json'

    result = analyze_tweets(data_file)

    # ä¿å­˜ç»“æœ
    output_file = data_file.replace('raw_data.json', 'analysis_summary.json')
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… åˆ†æå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜: {output_file}")

    # æ‰“å°æ‘˜è¦
    print(f"\nğŸ“‹ æ‘˜è¦:")
    print(f"  - æ€»æ¨æ–‡æ•°: {result['summary']['total_tweets']}")
    print(f"  - è¯†åˆ«äº§å“: {result['summary']['unique_products']}ä¸ª")
    print(f"  - æ–°äº§å“: {result['summary']['new_products']}ä¸ª")
    print(f"\nTop 10 æåŠæœ€å¤šçš„äº§å“:")
    for i, (product, info) in enumerate(list(result['products'].items())[:10], 1):
        print(f"  {i}. {product}: {info['mention_count']}æ¬¡æåŠ")
